<feed xmlns="http://www.w3.org/2005/Atom"><title>social media on Gabe</title><link href="https://gabriel.gaucimaistre.com/index.xml" rel="self"/><link href="https://gabriel.gaucimaistre.com/tags/social-media/"/><updated>2018-01-22T23:00:00+00:00</updated><id>https://gabriel.gaucimaistre.com/tags/social-media/</id><author><name>Gabriel Gauci Maistre</name></author><generator>Hugo -- gohugo.io</generator><entry><title type="html">Querying Amazon Athena Using Julia</title><link href="https://gabriel.gaucimaistre.com/2021/06/querying-amazon-athena-using-julia/"/><id>https://gabriel.gaucimaistre.com/2021/06/querying-amazon-athena-using-julia/</id><author><name>Gabriel Gauci Maistre</name></author><published>2021-06-08T23:00:00+00:00</published><updated>2021-06-08T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2021/06/querying-amazon-athena-using-julia/">&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/julia-loves-aws.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>Julia is a fairly modern scientific programming language that is free, high-level, fast, and bundles a bunch of awesome features that makes Julia working with data great again. The language borrows inspiration from languages such as Python, MATLAB and R&lt;a href="#f1">[1]&lt;/a>. If you haven&amp;rsquo;t yet read my article on &amp;ldquo;&lt;a href="https://gabriel.gaucimaistre.com/2018/09/10-reasons-why-you-should-learn-julia/">10 Reasons Why You Should Learn Julia&lt;/a>&amp;rdquo;, check it out! Amazon Athena is an interactive query service which allows you to easily analyze your data collecting dust in Amazon S3 storage using your good old friend SQL. Athena is great because it&amp;rsquo;s serverless, meaning there is no infrastructure to manage, and you pay only for the queries that you run.&lt;/p>
&lt;p>Sounds awesome right? Julia is great for working with data, Athena is great for querying data, how can we use both together? Rather than manually export CSV files and use &lt;code>CSV.jl&lt;/code> to load CSV files in Julia, I&amp;rsquo;ll be showing you how to query the data using Athena directly from Julia, loading the resulting set of data into a DataFrame using &lt;code>DataFrames.jl&lt;/code> for you to work with.&lt;/p>
&lt;p>New to Julia? Go ahead and grab the official binaries for your particular Operating System (OS) from the Julia site &lt;a href="https://julialang.org/downloads/#current_stable_release">here&lt;/a>. If you&amp;rsquo;re not interested in a detailed write-up but just want to see the intact code solution, check out &lt;a href="https://gist.github.com/gabegm/458288fa55437314b7b41b63a53a13a1">this&lt;/a> GitHub gist.&lt;/p>
&lt;h2 id="odbc">ODBC&lt;/h2>
&lt;p>Open Database Connectivity (ODBC) is a standard Application Programming Interface (API) for accessing Database Management Systems (DBMS). Because of its open standard nature, many databases support it, and many tools use it to connect to such databases. This means that the method of connecting with Amazon Athena shown in this post can actually be used for other databases too with just a few configuration changes that need to be made.&lt;/p>
&lt;p>AWS offers an ODBC driver for Athena which you may use to access your data. Make sure to install the corresponding ODBC driver for your OS from the official &lt;a href="https://docs.aws.amazon.com/athena/latest/ug/connect-with-odbc.html">user guide&lt;/a>. You may also optionally read the &lt;a href="https://s3.amazonaws.com/athena-downloads/drivers/ODBC/SimbaAthenaODBC_1.1.9.1001/docs/Simba+Athena+ODBC+Connector+Install+and+Configuration+Guide.pdf">documentation&lt;/a> for the ODBC driver should you run into any issues and/or need some extra configurations not covered in this post.&lt;/p>
&lt;h2 id="configurations">Configurations&lt;/h2>
&lt;p>We can use configuration files, or config for short, to configure the parameters and initial settings for our Julia applications. These parameters could range from Machine Learning model parameters to database credentials such as the ones in our example. Config files are widely used across many programming languages and provide a neat way of changing application settings without the need to change any code.&lt;/p>
&lt;p>For this example we&amp;rsquo;ll be using &lt;code>Configs.jl&lt;/code>&lt;a href="#f2">[2]&lt;/a> to load our config file which also supports cascading overrides based on the config location, &lt;code>ENV&lt;/code> variable mapping, and function calls.&lt;/p>
&lt;h2 id="dataframes">DataFrames&lt;/h2>
&lt;p>A DataFrame represents a table of data with rows and columns, just as spreadsheets do. The Julia package &lt;code>DataFrames.jl&lt;/code> provides an API we can use for working with tabular data in Julia. Similar to Pandas in Python, its design and functionality are quite similar, however thanks to Julia&amp;rsquo;s high degree of modularity, DataFrames in Julia are tightly integrated with a vast range of different packages such as &lt;code>Plots.jl&lt;/code>, &lt;code>MLJ.jl&lt;/code>, and &lt;a href="https://dataframes.juliadata.org/stable/#DataFrames.jl-and-the-Julia-Data-Ecosystem">many more&lt;/a>!&lt;/p>
&lt;h2 id="athenajl">Athena.jl&lt;/h2>
&lt;p>Enough chit-chat. Let&amp;rsquo;s start off by creating a new directory to house our code and name it &lt;code>Athena.jl&lt;/code>. We&amp;rsquo;ll also add a directly to store our database config file called &lt;code>configs&lt;/code>, another one to store our Julia code called &lt;code>src&lt;/code>, and one more to store our SQL script called &lt;code>sql&lt;/code> in our newly created &lt;code>src&lt;/code> directory. Finally, we&amp;rsquo;ll create three files, one called &lt;code>Athena.jl&lt;/code> which will be our Julia script, another called &lt;code>query.sql&lt;/code> for our SQL query, and the last called &lt;code>default.yml&lt;/code> for our database configurations and secrets.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">~ $ mkdir Athena.jl
~ $ cd Athena.jl
~/Athena.jl $ mkdir src src/sql configs
~/Athena.jl $ touch src/Athena.jl
~/Athena.jl $ touch src/sql/query.sql
~/Athena.jl $ touch configs/config.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Go ahead and open the &lt;code>Athena.jl&lt;/code> directory in your favourite IDE, mine is Visual Studio Code&lt;a href="#f3">[3]&lt;/a> which is what I used. Open the &lt;code>configs/default.yml&lt;/code> file so that we can add our configurations. You&amp;rsquo;ll need to add your &lt;code>s3 location&lt;/code>, &lt;code>uid&lt;/code>, and &lt;code>pwd&lt;/code>. If your AWS Cloud infrastructure is set up in a different region, you might also need to change the &lt;code>region&lt;/code> value. Based on the OS, you might also need to change the path to the database driver. If you&amp;rsquo;re running macOS such as myself, you might not need to change anything, just make sure the file exists in that directory.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yml" data-lang="yml">&lt;span style="color:#f92672">database&lt;/span>:
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;SimbaAthenaODBCConnector&amp;#34;&lt;/span>
&lt;span style="color:#f92672">path&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;/Library/simba/athenaodbc/lib/libathenaodbc_sb64.dylib&amp;#34;&lt;/span>
&lt;span style="color:#f92672">driver&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;SimbaAthenaODBCConnector&amp;#34;&lt;/span>
&lt;span style="color:#f92672">region&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;eu-west-1&amp;#34;&lt;/span>
&lt;span style="color:#f92672">s3_location&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;span style="color:#f92672">authentication_type&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;IAM Credentials&amp;#34;&lt;/span>
&lt;span style="color:#f92672">uid&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;span style="color:#f92672">pwd&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Add the SQL query you&amp;rsquo;d like to run in the &lt;code>query.sql&lt;/code> file in &lt;code>src/sql&lt;/code> and open the &lt;code>src/Athena.jl&lt;/code> Julia script to run the following in your REPL&lt;a href="#f4">[4]&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">&lt;span style="color:#66d9ef">import&lt;/span> Pkg
Pkg&lt;span style="color:#f92672">.&lt;/span>activate(&lt;span style="color:#e6db74">&amp;#34;.&amp;#34;&lt;/span>)
Pkg&lt;span style="color:#f92672">.&lt;/span>add([&lt;span style="color:#e6db74">&amp;#34;ODBC&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;DataFrames&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;Configs&amp;#34;&lt;/span>])
&lt;span style="color:#66d9ef">using&lt;/span> ODBC, DataFrames, Configs
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The first thing you might ask if you&amp;rsquo;re new to Julia is &amp;ldquo;what&amp;rsquo;s the difference between &lt;code>import&lt;/code> and &lt;code>using&lt;/code>?&amp;rdquo; Well &lt;code>import&lt;/code> works the same as in Python, &lt;code>import MyModule&lt;/code> would bring into scope&lt;a href="#f5">[5]&lt;/a> functions which will remain accessible using &lt;code>MyModule&lt;/code>, such as &lt;code>MyModule.x&lt;/code> and &lt;code>MyModule.y&lt;/code>, kind of like using &lt;code>import numpy&lt;/code> in Python and then running &lt;code>numpy.array([])&lt;/code>. However, &lt;code>using&lt;/code> in Julia is equivalent to running &lt;code>from numpy import *&lt;/code> in Python, which would bring all of Numpy&amp;rsquo;s functions into scope, allowing you to run &lt;code>array([])&lt;/code> in Python. Should you wish to only import specific functions into scope in Julia, all you need to do is &lt;code>import MyModule: x, y&lt;/code> which would now make functions &lt;code>x&lt;/code> and &lt;code>y&lt;/code> accessible in scope.&lt;/p>
&lt;p>&lt;code>Pkg&lt;/code> is the package manager bundled with Julia. We can not only use it to install packages but to also create virtual environments&lt;a href="#f6">[6]&lt;/a> to run our code from. By running &lt;code>Pkg.activate(&amp;quot;.&amp;quot;)&lt;/code> we are telling Julia&amp;rsquo;s package manager to activate a new virtual environment in the current directly for us to install our Julia dependencies in. This will automatically create two new files, &lt;code>project.toml&lt;/code> and &lt;code>Manifest.toml&lt;/code>, the former will list all our direct dependencies while the latter will list the indirect dependencies which our direct dependencies will rely on. These two files will allow any other developer to recreate the same virtual environment as ours which will be neat for reproducing this example.&lt;/p>
&lt;p>Next will use the &lt;code>Pkg.add&lt;/code> function to add a list of Julia packages we&amp;rsquo;d like to install, and use the &lt;code>using&lt;/code> command to import them into scope. Now that we&amp;rsquo;ve set up our Julia environment with the dependencies we will need to run this example, we can begin loading configurations from our config file using &lt;code>Configs.jl&lt;/code>&amp;rsquo;s &lt;code>getconfig&lt;/code> function. This function returns a &lt;code>NamedTuple&lt;/code> which is a Julia type that has two parameters: a tuple of symbols giving the field names, and a tuple type giving the field types. This means that we can use the field names from the config file itself to access them directly using the &lt;code>.&lt;/code> parameter. In the last step we&amp;rsquo;ll read the contents of the SQL script into Julia and parse them as a string for us to use later.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">database &lt;span style="color:#f92672">=&lt;/span> getconfig(&lt;span style="color:#e6db74">&amp;#34;database&amp;#34;&lt;/span>)
name &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>name
path &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>path
driver &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>driver
region &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>region
s3_location &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>s3_location
authentication_type &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>authentication_type
uid &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>uid
pwd &lt;span style="color:#f92672">=&lt;/span> database&lt;span style="color:#f92672">.&lt;/span>pwd
sql &lt;span style="color:#f92672">=&lt;/span> open(&lt;span style="color:#e6db74">&amp;#34;src/sql/query.sql&amp;#34;&lt;/span> ) &lt;span style="color:#66d9ef">do&lt;/span> file
read(file, &lt;span style="color:#66d9ef">String&lt;/span>)
&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before we begin querying Athena, &lt;code>ODBC.jl&lt;/code> requires that we add the Athena driver we installed earlier by passing it the name and path to the driver. We also need to build the connection string which we will use to connect to Athena.
Julia has native support for string interpolation allowing us to construct strings using any variable(s) which we may need without the need for concatenation&lt;a href="#f7">[7]&lt;/a>. The connection string is specific to the database you&amp;rsquo;re using, so if you won&amp;rsquo;t be connecting to Athena you&amp;rsquo;ll have to look up the documentation for the driver you&amp;rsquo;re using to construct the connection string required.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">&lt;span style="color:#75715e"># locate existing ODBC driver shared libraries or download new, then configure&lt;/span>
ODBC&lt;span style="color:#f92672">.&lt;/span>adddriver(name, path)
&lt;span style="color:#75715e"># build connection string&lt;/span>
connection_string &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;span style="color:#e6db74">Driver=&lt;/span>&lt;span style="color:#e6db74">$driver&lt;/span>&lt;span style="color:#e6db74">;
&lt;/span>&lt;span style="color:#e6db74">AwsRegion=&lt;/span>&lt;span style="color:#e6db74">$region&lt;/span>&lt;span style="color:#e6db74">;
&lt;/span>&lt;span style="color:#e6db74">S3OutputLocation=&lt;/span>&lt;span style="color:#e6db74">$s3_location&lt;/span>&lt;span style="color:#e6db74">;
&lt;/span>&lt;span style="color:#e6db74">AuthenticationType=&lt;/span>&lt;span style="color:#e6db74">$authentication_type&lt;/span>&lt;span style="color:#e6db74">;
&lt;/span>&lt;span style="color:#e6db74">UID=&lt;/span>&lt;span style="color:#e6db74">$uid&lt;/span>&lt;span style="color:#e6db74">;
&lt;/span>&lt;span style="color:#e6db74">PWD=&lt;/span>&lt;span style="color:#e6db74">$pwd&lt;/span>&lt;span style="color:#e6db74">;
&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Okay, I promise the fun part is coming. Now that we&amp;rsquo;ve finished setting up the boring configurations, we can go ahead and establish a connection and query Athena!&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">conn &lt;span style="color:#f92672">=&lt;/span> DBInterface&lt;span style="color:#f92672">.&lt;/span>connect(ODBC&lt;span style="color:#f92672">.&lt;/span>Connection, connection_string)
&lt;span style="color:#75715e"># execute sql statement directly, then materialize results in a DataFrame&lt;/span>
df &lt;span style="color:#f92672">=&lt;/span> DBInterface&lt;span style="color:#f92672">.&lt;/span>execute(
conn,
sql
) &lt;span style="color:#f92672">|&amp;gt;&lt;/span> DataFrame
&lt;span style="color:#ae81ff">297&lt;/span>&lt;span style="color:#f92672">×&lt;/span>&lt;span style="color:#ae81ff">3&lt;/span> DataFrame
Row │ dt table_name n_rows
│ DateTime…&lt;span style="color:#f92672">?&lt;/span> &lt;span style="color:#66d9ef">String&lt;/span>&lt;span style="color:#f92672">?&lt;/span> &lt;span style="color:#66d9ef">Int64&lt;/span>&lt;span style="color:#f92672">?&lt;/span>
─────┼────────────────────────────────────────────────
&lt;span style="color:#ae81ff">1&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">05&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">08&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">24.183&lt;/span> Table_A &lt;span style="color:#ae81ff">196040&lt;/span>
&lt;span style="color:#ae81ff">2&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">05&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">08&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">24.183&lt;/span> Table_B &lt;span style="color:#ae81ff">28172242&lt;/span>
&lt;span style="color:#ae81ff">3&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">05&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">08&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">24.183&lt;/span> Table_C &lt;span style="color:#ae81ff">27111764&lt;/span>
&lt;span style="color:#ae81ff">4&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">05&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">06&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">29.916&lt;/span> Table_A &lt;span style="color:#ae81ff">196041&lt;/span>
&lt;span style="color:#ae81ff">5&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">05&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">06&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">29.916&lt;/span> Table_C &lt;span style="color:#ae81ff">27080936&lt;/span>
&lt;span style="color:#ae81ff">6&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">05&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">23&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">26.201&lt;/span> Table_A &lt;span style="color:#ae81ff">196034&lt;/span>
&lt;span style="color:#f92672">⋮&lt;/span> │ &lt;span style="color:#f92672">⋮&lt;/span> &lt;span style="color:#f92672">⋮&lt;/span>
&lt;span style="color:#ae81ff">293&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">03&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">03&lt;/span>T14&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">47&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">56.910&lt;/span> Table_B &lt;span style="color:#ae81ff">27421193&lt;/span>
&lt;span style="color:#ae81ff">294&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">03&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">03&lt;/span>T14&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">47&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">56.910&lt;/span> Table_C &lt;span style="color:#ae81ff">26379105&lt;/span>
&lt;span style="color:#ae81ff">295&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">04&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">27&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">34.887&lt;/span> Table_A &lt;span style="color:#ae81ff">196046&lt;/span>
&lt;span style="color:#ae81ff">296&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">04&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">27&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">34.887&lt;/span> Table_B &lt;span style="color:#ae81ff">28016354&lt;/span>
&lt;span style="color:#ae81ff">297&lt;/span> │ &lt;span style="color:#ae81ff">2021&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">04&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">27&lt;/span>T06&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">46&lt;/span>&lt;span style="color:#f92672">:&lt;/span>&lt;span style="color:#ae81ff">34.887&lt;/span> Table_C &lt;span style="color:#ae81ff">26960853&lt;/span>
&lt;span style="color:#ae81ff">286&lt;/span> rows omitted
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Easy right? Julia was also able to parse the types of the columns in our DataFrame without any headaches. You can now run free plotting using &lt;code>Plots.jl&lt;/code>, train Machine Learning models using &lt;code>MLJ.jl&lt;/code>, and storing the transformed data back in Athena.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">&lt;span style="color:#75715e"># load data into database table&lt;/span>
ODBC&lt;span style="color:#f92672">.&lt;/span>load(df, conn, &lt;span style="color:#e6db74">&amp;#34;table_nme&amp;#34;&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Could you have done all of this in Python? Absolutely, in fact there are many posts like these which you can find on how to do that. However, I think Julia is an interesting language, &lt;a href="https://gabriel.gaucimaistre.com/2018/09/10-reasons-why-you-should-learn-julia/">is worth learning&lt;/a>, and can provide many benefits over Python.&lt;/p>
&lt;p>However, Julia is still a fairly new language, and although it&amp;rsquo;s steadily rising in popularity&lt;a href="(#f8)">[8]&lt;/a>&lt;a href="(#f9)">[9]&lt;/a>, still lacks the tooling and community support which a more widely used language such as Python provides. Looking at StackOverflow, the number of questions regarding Python dwarf the number of questions on Julia. It might be tempting to chalk this up to Julia being easier to use but — as much as I value Julia over Python — this isn&amp;rsquo;t the case. This is an example of selection bias, where the actual reason that the number of questions on Python versus the number of questions on Julia is significantly higher is simply because there are more people using Python than Julia. This underrepresentation of community questions and resources on StackOverflow actually contributes to Julia&amp;rsquo;s steeper learning curve.&lt;/p>
&lt;p>It is not, however, all doom and gloom. I do think that Julia, as a language, is easier to read than Python since most packages Julia are written in pure Julia. As popular as Python may be, it doesn&amp;rsquo;t always mean it&amp;rsquo;s the best tool for everyone or every job. Tooling and community support will, eventually, catch up as Julia&amp;rsquo;s popularity continues to rise and its user base continues to grow. Ultimately, what language best fits your needs is up to you.&lt;/p>
&lt;p>If you do however find yourself in the situation wanting to use a Python package not available in Julia, &lt;code>PyCall.jl&lt;/code> is a great package which provides you with the ability to directly call and fully interoperate&lt;a href="#f10">[10]&lt;/a> with Python from the Julia language. It&amp;rsquo;s as simple as:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">&lt;span style="color:#66d9ef">import&lt;/span> Pkg
Pkg&lt;span style="color:#f92672">.&lt;/span>activate(&lt;span style="color:#e6db74">&amp;#34;.&amp;#34;&lt;/span>)
Pkg&lt;span style="color:#f92672">.&lt;/span>add(&lt;span style="color:#e6db74">&amp;#34;PyCall&amp;#34;&lt;/span>)
&lt;span style="color:#66d9ef">using&lt;/span> PyCall
&lt;span style="color:#75715e"># create a variable &amp;#39;x&amp;#39; of type Float64&lt;/span>
x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
x &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">Float64&lt;/span>(x)
&lt;span style="color:#75715e"># use Anaconda to install scipy and it&amp;#39;s dependencies&lt;/span>
so &lt;span style="color:#f92672">=&lt;/span> pyimport_conda(&lt;span style="color:#e6db74">&amp;#34;scipy.optimize&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;scipy&amp;#34;&lt;/span>)
&lt;span style="color:#75715e"># use scipy&amp;#39;s newton function to find a zero of sin(x) given a nearby starting point 1&lt;/span>
so&lt;span style="color:#f92672">.&lt;/span>newton(x &lt;span style="color:#f92672">-&amp;gt;&lt;/span> cos(x) &lt;span style="color:#f92672">-&lt;/span> x, &lt;span style="color:#ae81ff">1&lt;/span>) &lt;span style="color:#75715e"># 0.7390851332151607&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;ul>
&lt;li>&lt;a name="f1">[1]&lt;/a> &lt;a href="https://cheatsheets.quantecon.org/">QuantEcon&lt;/a> has a great table comparing MATLAB, Python, and Julia.&lt;/li>
&lt;li>&lt;a name="f2">[2]&lt;/a> &lt;a href="https://github.com/citkane/Configs">Configs.jl&lt;/a> is an open source Julia package maintained by &lt;a href="https://github.com/citkane">Michael Jonker&lt;/a>.&lt;/li>
&lt;li>&lt;a name="f3">[3]&lt;/a> Visual Studio Code has an excellent &lt;a href="https://www.julia-vscode.org/">extension&lt;/a> to aid you in developing Julia applications maintained by the Julia community.&lt;/li>
&lt;li>&lt;a name="f4">[4]&lt;/a> A read–eval–print loop (REPL), is a simple interactive environment that takes code input, executes them, and returns the results.&lt;/li>
&lt;li>&lt;a name="f5">[5]&lt;/a> The scope of a variable is the region of code within which a variable is visible. Variable scoping helps avoid variable naming conflicts.&lt;/li>
&lt;li>&lt;a name="f6">[6]&lt;/a> Virtual environments help us ensure our applications and its dependencies remain independent from other applications by avoiding any conflicts in versions which may arise.&lt;/li>
&lt;li>&lt;a name="f7">[7]&lt;/a> In Julia, strings would be concatenated by using the &lt;code>*&lt;/code> operator: &lt;code>&amp;quot;Hello &amp;quot; * &amp;quot;world&amp;quot;&lt;/code>&lt;/li>
&lt;li>&lt;a name="f8">[8]&lt;/a> &lt;a href="https://lwn.net/Articles/834571/">The accelerating adoption of Julia&lt;/a>&lt;/li>
&lt;li>&lt;a name="f9">[9]&lt;/a> &lt;a href="https://www.hpcwire.com/2021/01/13/julia-update-adoption-keeps-climbing-is-it-a-python-challenger/">Julia Update: Adoption Keeps Climbing; Is It a Python Challenger?&lt;/a>&lt;/li>
&lt;li>&lt;a name="f10">[10]&lt;/a> Julia has excellent interop with not only Python but a vast amount of &lt;a href="https://github.com/JuliaInterop">other languages&lt;/a>.&lt;/li>
&lt;/ul></content></entry><entry><title type="html">On Failed Machine Learning Experiments</title><link href="https://gabriel.gaucimaistre.com/2021/06/on-failed-machine-learning-experiments/"/><id>https://gabriel.gaucimaistre.com/2021/06/on-failed-machine-learning-experiments/</id><author><name>Gabriel Gauci Maistre</name></author><published>2021-06-06T23:00:00+00:00</published><updated>2021-06-06T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2021/06/on-failed-machine-learning-experiments/">&lt;p>&lt;em>&lt;strong>DISCLAIMER&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>The following are simply my views and in no way reflect those of the whole industry.&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/i-have-no-idea-what-im-doing.jpg" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>The &amp;ldquo;science&amp;rdquo; in Data Science is supposed to refer to the fact that doing Data Science involves conducting experiments with data&lt;a href="#f0">[0]&lt;/a>. In any other field failed experiments are accepted, you wouldn&amp;rsquo;t want a drug company to push out a new drug which failed internal tests, so why is it frowned upon in Data Science?&lt;/p>
&lt;p>There is this sense in industry that a failed experiment equates to a failed project, thus Data Scientists are incentivised to not conduct experiments which end up in failure. This results in either Data Scientists not advertising experiments as failures and instead adding model complexity to cover up the fact that the model was just overfit on in-sample data&lt;a href="#f1">[1]&lt;/a>, or not attempting novel experiments in case they do fail. This isn&amp;rsquo;t to say that all Data Science experiments should fail, if that is the case then there is something fundamentally wrong with the type of experiments that are being pursued.&lt;/p>
&lt;h2 id="how-machine-learning-experiments-are-conceptualised">How Machine Learning experiments are conceptualised&lt;/h2>
&lt;p>Normally in a data-driven organisation, either a Product Owner (PO)&lt;a href="#f2">[2]&lt;/a> or a member of the board comes up with an application for Machine Learning within the organisation. This idea could have either come from having witnessed others applying Machine Learning to the same problem within their own organisation&lt;a href="#f3">[3]&lt;/a>, or a novel idea which they came up with to try&lt;a href="#f4">[4]&lt;/a>. Maybe one of the investors gives the board an objective to implement &amp;ldquo;Artificial Intelligence&amp;rdquo; (AI) to some part of the product and it&amp;rsquo;s now the board&amp;rsquo;s job to make that happen. The PO&amp;rsquo;s job is to do some research and find out how others were able to apply this to their product, how to integrate such a feature within the PO&amp;rsquo;s product, and approach a Data Scientist to attempt the same at the organisation. The PO secures a budget from the board to deliver this project into production against a specific deadline.&lt;/p>
&lt;p>This is often where the first issue occurs, the budget was approved without first awaiting the preliminary results from the Data Scientist to see whether there is a Machine Learning application for such a problem within the organisation. Another common mistake takes place when the deadline was set without even discussing the work required with the Data Scientist who is ultimately doing the work. This is at a stage where the Data Scientist has not yet conducted any proper analyses of the data and does not know what to expect.&lt;/p>
&lt;p>The job of the Data Scientist is to now do a more in-depth research into how Machine Learning was applied to the same problem at other companies or industries and form an understanding on how it could be applied at their organisation. Once the Data Scientist has achieved a thorough understanding of the approach, the next step would be to attempt to recreate the experiment using the code and data from the original experiment, assuming this has been shared publicly. If not, the Data Scientist must move on to formulate an experiment using the data belonging to the organisation. Thorough analysis of the organisation&amp;rsquo;s data would need to be done to look for any trends which might suggest such a Machine Learning model would be applicable. At this point the Data Scientist would already have a clear understanding on whether this experiment will turn out successful or not. This is where the road for the Data Scientist now forks.&lt;/p>
&lt;p>If the Data Scientist works for an organisation that is open to failed experiments, they will come forward and openly share the full results of the analysis thus far and express their concerns and why they think this experiment will not pan out. Depending on the receiving audience, some technical discussions might take place and this might give the Data Scientist some ideas to go back to the drawing board and try again. If the experiment cannot be replicated using the organisation&amp;rsquo;s data, either because the data is not good enough, is not available to them, does not follow the same trends, or the original experiment was flawed, then that&amp;rsquo;s another lesson learnt and the Data Scientist moves onto the next potential experiment.&lt;/p>
&lt;p>However, if the Data Scientist works for an organisation that does not accept failed experiments which the Data Scientist is well aware of, they will push forward and try anything that sticks in order to not show up empty handed. The Data Scientist will present to their audience the results of the analysis which they are expecting to see and thus the project will be green lit to move forward.&lt;/p>
&lt;p>At this point the Data Scientist knows that they will not be able reproduce the results from the experiment using the organisation&amp;rsquo;s data but will have to keep pushing forward and try anything that will work.&lt;/p>
&lt;h2 id="frankenstein-machine-learning">Frankenstein Machine Learning&lt;/h2>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/its-alive.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>This is where things get ugly. After weeks of hacking away, the Data Scientist finds some black box model that seems to perform what everyone expects. However, the Data Scientist is not able to explain what the model is doing, neither is anyone going to ask because no one wants to see this project fail. The results are presented to the PO or the board and this is when things spiral out of control and the results are publicised in some internal blog about how Artificial Intelligence was applied to an organisation problem, or some marketing post on LinkedIn.&lt;/p>
&lt;p>After a while the Machine Learning model gets deployed to production and suddenly does not perform well because the model was overfit to the training data and is now struggling with all of this out-of-sample data. At this rate either of two things happen. Either the Data Scientist comes under fire for delivering something that does not work, or no one bothers to raise any concerns because again no one wants to label this a failed project.&lt;/p>
&lt;h2 id="data-scientists-are-too-scared-to-try-novel-experiments-in-case-they-fail">Data Scientists are too scared to try novel experiments in case they fail&lt;/h2>
&lt;p>Data Science innovation can&amp;rsquo;t grow within an organisation if it&amp;rsquo;s hindered by the fear of failure. If Machine Learning applications are to advance within the organisation, Data Scientists must be free to try out new things without the fear of failure hanging over their head. Let&amp;rsquo;s take an e-commerce business as an example. Some executive comes up with the idea that it is possible to use Machine Learning to recommend clothes sizes to customers. This experiment gets green lit and the Data Scientist starts their research. Maybe they find out that there is no way to recommend clothes sizes to a customers because customers tend to not choose the same size for each type of clothing. Maybe it&amp;rsquo;s because the customer often buys various types of clothes and thus no trend can be derived, or maybe it&amp;rsquo;s because different clothes manufacturers have varying sizes for the same types of clothing. One thing that the Data Scientist did not consider is that maybe it&amp;rsquo;s also discriminatory to preselect a size for a customer, something which the PO should have flagged before this experiment ever saw the light of day. After all no one likes to be reminded to wear 5XL.&lt;/p>
&lt;p>The Data Scientist knows that the business is expecting results, so they push forward and come up with a model that &amp;ldquo;performs well&amp;rdquo; using in-sample data. The Data Scientist is happy that they were able to produce something management will like, management are happy because they can say they applied &amp;ldquo;AI&amp;rdquo; to their e-commerce product. Fast forward a couple of months and this feature is eventually rolled back because too many customers complained that they were either recommended the incorrect size or they didn&amp;rsquo;t like the choice being made for them. The feature provided no improvement to the customer and all of this work could have been spared for something more effective.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/data-scientist-look-away.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>If management were open to a failed experiment from the start, the Data Scientist would have gone back to them with their results and they would have concluded that it is not possible or valuable to apply Machine Learning to this problem at the time and they might need to revisit this at a later stage or rule it out completely. The board can now look for the next problem to try and solve with Machine Learning without wasting months of worthless development, potentially with a successful application this time.&lt;/p>
&lt;h2 id="progression-should-not-be-tied-to-successful-experiments">Progression should not be tied to successful experiments&lt;/h2>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/denied-pay-rise.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>If Data Scientists are to be allowed to fail at their experiments, their compensation and progression cannot be tied to the success of such experiments. &amp;ldquo;Deliver 4 models to production in 2021&amp;rdquo; is the best way to incentivise a Data Scientist to do whatever they can to make sure their experiments do not &amp;ldquo;fail&amp;rdquo;, whatever the cost.&lt;/p>
&lt;p>Instead, progression should be tied towards the experiments themselves and the quality of their results, whether those results deem the experiment to be successful or not. The essential thing here is that the organisation learns from its data because the Data Scientist is able to answer the questions which they may have, by conducting the rigorous experiments required, in which the management accepts the results, whether or not they follow their gut feeling.&lt;/p>
&lt;h2 id="data-science-cannot-be-about-budgets">Data Science cannot be about budgets&lt;/h2>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/product-owner.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>Far too often is the case that large budgets are tied to Data Science projects. The PO secures a huge budget to use towards applying Machine Learning towards a problem within the organisation. It&amp;rsquo;s in the PO&amp;rsquo;s best interest that the project succeeds as they do not wish to lose their budget which could go towards improved infrastructure and new hires which would be both desperately needed.&lt;/p>
&lt;p>Instead, budgets should only be assigned once the results from the Data Science experiments are completed and have gone through extensive review by peers in order to have a stronger foundation for presenting the results to the board and ultimately apply a Machine Learning model towards a specific part of the organisation&amp;rsquo;s product to increase its value.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Data Science projects are different to Software Engineering projects. Machine Learning applications heavily depend on the data available and the product to which it would be applied and the customers interacting with it. Data Science relies heavily on experiments and businesses should be open towards results from experiments which may not be in line with what they originally thought to be true.&lt;/p>
&lt;p>Successful experiments are not those which prove a hypothesis to be true, but one which is carried out fairly to prove a particular hypothesis to be true or not. Ultimately many lessons are learnt from such experiments which is why it&amp;rsquo;s always a good idea to keep an open mind and to always question things but to never believe something blindly.&lt;/p>
&lt;p>I tried my best to keep my examples as vague as possible so that they can be applied to any organisation working with data, no matter the extent of Machine Learning experiments being carried out within the organisation. This should hopefully aid in the self reflection required to see that Machine Learning experiments are done right, rather than fuelling a debate of good v.s. bad Machine Learning experiments. If applied correctly, Machine Learning can provide vast amounts of benefits to an organisation&amp;rsquo;s product(s) and its customers, which is why it&amp;rsquo;s always a good exercise to have these kinds of discussions of self-reflection.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;a name="f0">[0]&lt;/a> Testing whether it&amp;rsquo;s possible to apply Machine Learning to recommend a clothing brand to a customer using the data available to the Data Scientist.&lt;/li>
&lt;li>&lt;a name="f1">[1]&lt;/a> In-sample data refers to the data available used to train and test a Machine Learning model to predict on data not yet observed.&lt;/li>
&lt;li>&lt;a name="f2">[2]&lt;/a> The Product Owner, while responsible for maximizing the value of the product and the work of the Development Team, is the sole person responsible for managing the Product Backlog.&lt;/li>
&lt;li>&lt;a name="f3">[3]&lt;/a> Commonly witnessed through conferences, webinars, and articles.&lt;/li>
&lt;li>&lt;a name="f4">[4]&lt;/a> &amp;ldquo;Let&amp;rsquo;s recommend items on the food menu for our customers based on their age!&amp;rdquo;&lt;/li>
&lt;/ul></content></entry><entry><title type="html">Data Science &amp;lt;3 Clean Code</title><link href="https://gabriel.gaucimaistre.com/2021/05/data-science-3-clean-code/"/><id>https://gabriel.gaucimaistre.com/2021/05/data-science-3-clean-code/</id><author><name>Gabriel Gauci Maistre</name></author><published>2021-05-20T23:00:00+00:00</published><updated>2021-05-20T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2021/05/data-science-3-clean-code/">&lt;p>&lt;em>&lt;strong>DISCLAIMER&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>The following are simply my views and in no way reflect those of the whole industry.&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/data-scientists-should-write-clean-code.png" alt="alt text" title="Data Scientists should write clean code; change my mind.">
&lt;em>I&amp;rsquo;m back with another controversial topic!&lt;/em>&lt;/p>
&lt;p>There is a common misconception among people with an academic background that being a Data Scientist means you do not need to care about writing good clean code, because you&amp;rsquo;re not a Software Developer or a Machine Learning Engineer. Data Science is all about running experiments on data and you should not need to spend your precious time worrying about making things look pretty. If you&amp;rsquo;re a firm believer of this, please take some time to hear me out. I hope to have at least gotten you to reconsider by the end of this post.&lt;/p>
&lt;h2 id="software-development-principles-arent-for-software-developers">Software Development principles aren&amp;rsquo;t for Software Developers&lt;/h2>
&lt;p>Software Development principles might have been curated by Software Developers to aid in the creation of software, but they definitely were not meant &lt;em>just&lt;/em> for Software Developers. They were proposed for anyone writing any kind of software, this of course includes Data Scientists. Below are a few which I felt were worth mentioning.&lt;/p>
&lt;h3 id="linting">Linting&lt;/h3>
&lt;p>Python is not a compiled language, which means there is no way of knowing whether your code will execute successfully unless you attempt to execute it and the interpreter[0] finds a problem which halts the execution of your code. This means that it&amp;rsquo;s extremely important to use a linter which is a tool that will help finding bugs and style problems in your code. It&amp;rsquo;s important to note that it&amp;rsquo;s not perfect due to Python&amp;rsquo;s dynamic nature[1], however false warnings should be fairly infrequent.&lt;/p>
&lt;p>In the following example, Pylint[2] will let us know that our script has some issues such as lines being too long, have missing whitespace characterss and unreachable code.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ pylint my_script.py
************* Module pylint.checkers.format
W: 50: Too long line &lt;span style="color:#f92672">(&lt;/span>86/80&lt;span style="color:#f92672">)&lt;/span>
W:108: Operator not followed by a space
print &amp;gt;&amp;gt;sys.stderr, &lt;span style="color:#e6db74">&amp;#39;Unable to match %r&amp;#39;&lt;/span>, line
^
W:141: Too long line &lt;span style="color:#f92672">(&lt;/span>81/80&lt;span style="color:#f92672">)&lt;/span>
W: 74:searchall: Unreachable code
W:171:FormatChecker.process_tokens: Redefining built-in &lt;span style="color:#f92672">(&lt;/span>type&lt;span style="color:#f92672">)&lt;/span>
W:150:FormatChecker.process_tokens: Too many local variables &lt;span style="color:#f92672">(&lt;/span>20/15&lt;span style="color:#f92672">)&lt;/span>
W:150:FormatChecker.process_tokens: Too many branches &lt;span style="color:#f92672">(&lt;/span>13/12&lt;span style="color:#f92672">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="documentation">Documentation&lt;/h3>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/i-feel-bad-for-you.png" alt="alt text" title="Machine Learning Engineers feel bad for Data Scientists but they don't care">&lt;/p>
&lt;p>Documentation is not needed to develop software, neither are comments and docstrings. All of which take more time and effort to do. However they will save your time in the long run as your codebase grows larger and more people other than yourself will have to look at your code, maintain it, or even extend it. It goes without saying that it will also help you maintain your own codebase.&lt;/p>
&lt;p>The following is an example of how to write docstrings for your Python functions.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;A one line summary of the module or program, terminated by a period.
&lt;/span>&lt;span style="color:#e6db74">
&lt;/span>&lt;span style="color:#e6db74">Leave one blank line. The rest of this docstring should contain an
&lt;/span>&lt;span style="color:#e6db74">overall description of the module or program. Optionally, it may also
&lt;/span>&lt;span style="color:#e6db74">contain a brief description of exported classes and functions and/or usage
&lt;/span>&lt;span style="color:#e6db74">examples.
&lt;/span>&lt;span style="color:#e6db74">
&lt;/span>&lt;span style="color:#e6db74"> Typical usage example:
&lt;/span>&lt;span style="color:#e6db74">
&lt;/span>&lt;span style="color:#e6db74"> foo = ClassFoo()
&lt;/span>&lt;span style="color:#e6db74"> bar = foo.FunctionBar()
&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Google&amp;rsquo;s &lt;a href="https://google.github.io/styleguide/pyguide.html#381-docstrings">Python style guide&lt;/a>&lt;/em>&lt;/p>
&lt;p>Type hinting is also another form of documentation you may add to your code which also has no effect on its execution but is meant to make it more legible. Specified in &lt;a href="https://www.python.org/dev/peps/pep-0484/">PEP 484&lt;/a> and introduced in &lt;a href="https://docs.python.org/3.5/whatsnew/3.5.html">Python 3.5&lt;/a>, type hints allow you to annotate your Python code with hints, hence the name, as to what types your function arguments, return values, and variables are using.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">fib&lt;/span>(n):
a, b &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">while&lt;/span> a &lt;span style="color:#f92672">&amp;lt;&lt;/span> n:
&lt;span style="color:#66d9ef">yield&lt;/span> a
a, b &lt;span style="color:#f92672">=&lt;/span> b, a&lt;span style="color:#f92672">+&lt;/span>b
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Just by looking at this code block, it is hard to tell what types the function is expecting unless we run it. For small codebases this might be okay, but as you can imagine it would be quite counterproductive to have to stay running each and every function when we want to get a better idea of which types are required.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#f92672">from&lt;/span> typing &lt;span style="color:#f92672">import&lt;/span> Iterator
&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">fib&lt;/span>(n: int) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> Iterator[int]:
a, b &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#66d9ef">while&lt;/span> a &lt;span style="color:#f92672">&amp;lt;&lt;/span> n:
&lt;span style="color:#66d9ef">yield&lt;/span> a
a, b &lt;span style="color:#f92672">=&lt;/span> b, a&lt;span style="color:#f92672">+&lt;/span>b
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Thanks to the type hints present in this function definition, we now know that the function expects an &lt;code>integer&lt;/code> variable, and returns an iterator consisting of &lt;code>integer&lt;/code> elements. None of this effects the execution of your code, but it makes it much easier to read Python code since types are not required.&lt;/p>
&lt;p>Static type checkers will also allow you to check your code for type errors making it easier to find bugs with less debugging. In the following example, mypy[3] will find a bug where we tried to pass a &lt;code>string&lt;/code> as an argument to the &lt;code>fib&lt;/code> function which was expecting an &lt;code>integer&lt;/code> instead. If we were to run this code, this bug would cause the execution to come to a halt.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ mypy my_script.py
my_script.py:10: error: Argument &lt;span style="color:#e6db74">&amp;#34;n&amp;#34;&lt;/span> to &lt;span style="color:#e6db74">&amp;#34;fib&amp;#34;&lt;/span> has incompatible
type &lt;span style="color:#e6db74">&amp;#34;str&amp;#34;&lt;/span>; expected &lt;span style="color:#e6db74">&amp;#34;int&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You may not care about making your code easier for others to read, but all of the things I mentioned above are supposed to make life easier for you. The larger your codebase grows, the harder it&amp;rsquo;ll be to maintain. Following these principles will make life a little easier. If you need some extra motivation, imagine that the next person to end up maintaining your code is a violent psychopath who knows where you live, that should do the trick.&lt;/p>
&lt;h2 id="consistency">Consistency&lt;/h2>
&lt;p>Blank lines and white spaces help make code more readable. Fixed line lengths ensure that your code is legible on all types of screens. However, even the best of intentions can be wasted by an inconsistent use of lines and white spaces, rendering even the most well-written code to be confusing. The antidote to inconsistency is the same as with any other form of writing: the style guide. Python has many style guides, your place of work might actually have one too.&lt;/p>
&lt;p>Unfortunately it is fairly common for each team within a company to follow their own style guide which makes it hard to standardise within a company. If your company does not yet have one, I suggest you find one that suits your needs and stick to it whenever you&amp;rsquo;re writing code.[4]&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">No:
&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">to_sql&lt;/span>(frame, name: str, con, schema: str &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>, if_exists: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;fail&amp;#34;&lt;/span>, index: bool &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>, index_label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>, chunksize: int &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>, dtype: DtypeArg &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>, method: str &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>, engine: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>, &lt;span style="color:#f92672">**&lt;/span>engine_kwargs,) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>
Yes:
&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">to_sql&lt;/span>(
frame,
name: str,
con,
schema: str &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
if_exists: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;fail&amp;#34;&lt;/span>,
index: bool &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>,
index_label&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">None&lt;/span>,
chunksize: int &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
dtype: DtypeArg &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
method: str &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>,
engine: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;span style="color:#f92672">**&lt;/span>engine_kwargs,
) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>An example of fixed line lengths&lt;/em>&lt;/p>
&lt;h2 id="naming">Naming&lt;/h2>
&lt;p>Whether you&amp;rsquo;re developing your own library or importing someone else&amp;rsquo;s, it&amp;rsquo;s essential to use conventional and meaningful names. This not only helps to stay consistent with the library&amp;rsquo;s conventions but also to make your code more clear and concise. Library names can also collide if you&amp;rsquo;re not careful so always be weary of what you&amp;rsquo;re importing.&lt;/p>
&lt;p>Importing &lt;code>numpy&lt;/code> as &lt;code>n&lt;/code> rather than &lt;code>np&lt;/code> which is the preferred alias and used by the community is not going to help others get up to speed with what you&amp;rsquo;re trying to achieve with your code.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/variable-naming.png" alt="alt text" title="Giving a variable a well thought out name, no. Naming it 'x', definitely.">&lt;/p>
&lt;p>This also goes for naming your variables and functions. A variable named &lt;code>x&lt;/code> isn&amp;rsquo;t going to help anyone understand what &lt;code>x&lt;/code> is meant to be storing and used for. You can never know when you&amp;rsquo;ll need to look at your own code again down the line and forget what &lt;code>x&lt;/code> was meant to be used for.&lt;/p>
&lt;h2 id="dumping-your-code-on-a-machine-learning-engineer-wont-work">Dumping your code on a Machine Learning Engineer won&amp;rsquo;t work&lt;/h2>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/dump-truck.png" alt="alt text" title="Data Scientists dumping their code onto Machine Learning Engineers">&lt;/p>
&lt;p>If Data Scientists are the head chefs in a restaurant forming new recipes, Machine Learning Engineers are the cooks attempting to recreate the particular dish using the recipe provided to them. If the cooks are to successfully recreate the dish, they&amp;rsquo;re going to need a well detailed recipe which is easy to read and understand.&lt;/p>
&lt;p>Machine Learning Engineers are no different to these cooks, if they are to deploy to production a Machine Learning model which a Data Science built, they must be able to understand the code and run it successfully, easily recreate the data that was used, install all the dependencies without running into any issues, and recreate the same results of the model.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/how-to-run-this-code.png" alt="alt text" title="Machine Learning Engineers asking Data Scientists how to run their code.">&lt;/p>
&lt;p>This is no easy task, imagine trying peak into someone else&amp;rsquo;s brain and understand the very thing which they&amp;rsquo;ve been working on for months. Now imagine that work did not consist of clean commented code, which is documented thoroughly, uses virtual environments to execute the code, and includes SQL queries to retrieve and recreate the data that was used to train the Machine Learning model. There&amp;rsquo;s a far greater chance of the results not being recreated as the Data Scientist had in mind, causing problems with the model running in production down the line.&lt;/p>
&lt;h2 id="you-shouldnt-tie-your-development-to-a-notebook">You shouldn&amp;rsquo;t tie your development to a notebook&lt;/h2>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/i-want-you-notebook.png" alt="alt text" title="Data Scientists want be with notebooks forever">&lt;/p>
&lt;p>The topic of notebooks[5] is so vast I could probably have a whole post dedicated to it, &lt;em>notes this down for later.&lt;/em> But to keep things short, although notebooks present a fantastic way of prototyping your code, they lack the features you would normally find in an Integrated Development Environment (IDE)[6].&lt;/p>
&lt;p>Notebooks don&amp;rsquo;t come with linting out of the box, which makes it hard to identify and correct subtle programming errors or unconventional coding practices that can lead to errors. A linter can tell you that you forgot to pass a variable in your function&amp;rsquo;s argument, or that there&amp;rsquo;s unused variables/functions in your code. All of which can make it even more confusing to understand the notebook code when reading it.&lt;/p>
&lt;p>Now you could make the argument that you could always prototype your code in a quick and dirty way and then rewrite it using the proper principles once you know what it is you want from your code. But you&amp;rsquo;re still going to have to go through your code, figure out what you had in mind at the time when you wrote it, and rewrite it again. It would be a lot easier if you followed the correct principles in the first place and had the proper documentation, such as comments and docstrings, to work off from.&lt;/p>
&lt;h2 id="a-better-way-to-use-notebooks">A better way to use notebooks&lt;/h2>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/data-science-two-buttons.png" alt="alt text" title="Use proper software development principles v.s. dumping all code in the notebook to a script.">
&lt;em>Use proper software development principles or just dump the notebook as code[7]&lt;/em>&lt;/p>
&lt;p>Notebooks are great for prototyping but they don&amp;rsquo;t help in following Software Development principles, which hopefully by now I have convinced you that you should use them. Wouldn&amp;rsquo;t it be great if you could have your cake and also eat it? Well in this case it&amp;rsquo;s possible. Here&amp;rsquo;s my approach.&lt;/p>
&lt;p>Everything I write goes into a function which gets packaged into a library. I develop this code in an IDE which gives me all the benefits of Software Development principles. As I&amp;rsquo;m doing so, I import my library in a notebook and prototype the code as I&amp;rsquo;m writing it. Jupyter notebooks also make this easy by allowing me to use magic commands such as &lt;code>%autoreload&lt;/code> which allows me to automatically reload libraries as I update them within my environment without the need to restart the kernel.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">[&lt;span style="color:#ae81ff">1&lt;/span>]: &lt;span style="color:#f92672">%&lt;/span>load_ext autoreload
&lt;span style="color:#f92672">%&lt;/span>autoreload &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#75715e"># tells jupyter to autoreload libraries as they&amp;#39;re updated in our environment&lt;/span>
[&lt;span style="color:#ae81ff">2&lt;/span>]: &lt;span style="color:#f92672">from&lt;/span> my_library &lt;span style="color:#f92672">import&lt;/span> fib &lt;span style="color:#75715e"># makes our function accessible&lt;/span>
[&lt;span style="color:#ae81ff">3&lt;/span>]: [print(i) &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> fib(&lt;span style="color:#ae81ff">5&lt;/span>)] &lt;span style="color:#75715e"># executes our function&lt;/span>
&lt;span style="color:#ae81ff">0&lt;/span>
&lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#ae81ff">1&lt;/span>
&lt;span style="color:#ae81ff">2&lt;/span>
&lt;span style="color:#ae81ff">3&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Whenever I need to change the code behind the function &lt;code>fib&lt;/code>, all I need to do is simply reinstall it by running &lt;code>$ pip install .&lt;/code>[8] in my terminal and simply rerunning cell &lt;code>3&lt;/code> above which will now use the function from my newly updated library.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>[0] An interpreter directly executes code without the need to have been previously compiled into a machine language program.&lt;/li>
&lt;li>[1] Python is dynamically typed, meaning the types of variables are only determined at runtime and do not need to be specified beforehand.&lt;/li>
&lt;li>[2] &lt;a href="https://pylint.org/">Pylint&lt;/a> is a source-code, bug and quality checker for the Python programming language.&lt;/li>
&lt;li>[3] &lt;a href="http://mypy-lang.org/">Mypy&lt;/a> is an optional static type checker for Python that aims to combine the benefits of dynamic (or &amp;ldquo;duck&amp;rdquo;) typing and static typing.&lt;/li>
&lt;li>[4] Google have a public &lt;a href="https://google.github.io/styleguide/pyguide.html">Python style guide&lt;/a> which I highly recommend to check out.&lt;/li>
&lt;li>[5] A Jupyter notebook consists of an interactive web tool known as a computational notebook, which researchers can use to combine software code, computational output, explanatory text, and multimedia resources in a single document.&lt;/li>
&lt;li>[6] An IDE normally consists of at least a source code editor, build automation tools, and a debugger.&lt;/li>
&lt;li>[7] &lt;a href="https://nbconvert.readthedocs.io/en/latest/">nbconvert&lt;/a> is a tool which lets you convert a notebook into executable code, as brilliant as that can sound, the tool doesn&amp;rsquo;t check for any parts of your code which are not being used or might cause problems. So think of it as a dump from one format to another.&lt;/li>
&lt;li>[8] &lt;code>.&lt;/code> refers to the current directory which in this case is the one where our library is located.&lt;/li>
&lt;/ul></content></entry><entry><title type="html">I Don&amp;#39;t Like Data Science Competitions</title><link href="https://gabriel.gaucimaistre.com/2021/05/i-dont-like-data-science-competitions/"/><id>https://gabriel.gaucimaistre.com/2021/05/i-dont-like-data-science-competitions/</id><author><name>Gabriel Gauci Maistre</name></author><published>2021-05-18T23:00:00+00:00</published><updated>2021-05-18T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2021/05/i-dont-like-data-science-competitions/">&lt;p>&lt;em>&lt;strong>DISCLAIMER&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>The following are simply my views and in no way reflect those of the whole industry.&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/audience-booing.gif" alt="alt text" title="Audience booing">&lt;/p>
&lt;h2 id="what-is-a-data-science-competition">What is a Data Science competition?&lt;/h2>
&lt;p>Data Science competitions have become extremely popular in the past 5 years. They are not only popular on sites such as &lt;a href="https://www.kaggle.com/">Kaggle&lt;/a>[0], but also when interviewing with companies for a role on the job market in the form of a &lt;a href="https://exercism.io/">candidate test&lt;/a>. These tests have quickly turned into a way to judge the performance of a participant/candidate&amp;rsquo;s submission and determine whether they have succeeded in the challenge or not.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/titanic-competition.png" alt="alt text" title="Kaggle titanic competition">
&lt;em>Popular titanic competition&lt;/em>&lt;/p>
&lt;p>These competitions/tests normally consist of a time constrained objective where the organiser provides some data, many times a train and test set[1], and participants submit the predictions which their model spat out in order to see how well they fared compared to all the other participants. That is, the predictions are compared with the dependant variable[2] omitted from the test set. Those yielding the highest accuracy fair much better compared to the rest. In certain cases the participant&amp;rsquo;s methods may also be questioned, but this is more the case in job interviews, although not as often as it should, rather than in online competitions.&lt;/p>
&lt;p>While my introduction to Data Science happened during my studies at university, it practice in the industry that helped me see that these are two worlds apart. This is not to say that education does not have its place in the space of Data Science and Machine Learning.&lt;/p>
&lt;p>My bachelor thesis revolved around algorithmic trading. I trained a Machine Learning model to pick correlated and inversely correlated stocks from a basket, forecast future price movements in said stocks, and decide on whether to hold, buy, or sell based on those insights without the need of any human input. Although I did learn a lot from this experience, I would definitely not use the model in industry and I would absolutely not let it use real money to invest.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/algorithmic-trading.jpg" alt="alt text" title="Stock market prediction">&lt;/p>
&lt;h2 id="real-data-does-not-come-in-a-csv-file">Real data does not come in a CSV file&lt;/h2>
&lt;p>Data sets found in industry are completely different compared to those found in competitions. In fact, I would not even go as far as to call them data sets but a huge database, sometimes multiple if you are not lucky enough to have a centralised data warehouse. You would need to sift through numerous tables, of which lacking proper documentation, looking for the data you need, only to then do some heavy tidying to structure your data, and thoroughly checking the data before you can even consider using it in a Machine Learning model.&lt;/p>
&lt;p>This is completely different to a &lt;code>.csv&lt;/code> file which you are handed in a competition, a data set where someone has already gone through the work of packaging the data for you. There is a joke that &amp;ldquo;Data Scientists spend 80% of their time cleaning data and the other 20% complaining about it.&amp;rdquo; This is of course an over exaggeration, however it is meant to emphasise how hard it is for a Data Scientist to get the data they need before they may even begin modelling. This is something which these competitions fail to teach you.&lt;/p>
&lt;h2 id="industry-is-not-about-producing-the-highest-fraction-of-accuracy">Industry is not about producing the highest fraction of accuracy&lt;/h2>
&lt;p>When you are dealing with companies and their customers, it is more important to understand how a model is forming its decisions rather than the highest accuracy it can achieve. Ultimately a weak model can hurt a company&amp;rsquo;s business and their image, potentially at a loss of both their customers and money. A Machine Learning model may achieve excellent results in a test set, however that does not mean that its assumptions will hold once the model is deployed in production and tested against real data. What will your model do when it finds an example which its never come across before?&lt;/p>
&lt;p>It is also possible that the model has been &lt;a href="https://en.oxforddictionaries.com/definition/overfitting">overfit&lt;/a>[3] on the training data, performing excellently in test environments but less so in production. In other cases the train and test sets could also include biases introduced by humans allowing the Machine Learning model to learn from the same biases and apply them in its decisions.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/amazon-ai-scandal.png" alt="alt text" title="Amazon AI scandal">
&lt;em>&lt;a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G">reuters&lt;/a>&lt;/em>&lt;/p>
&lt;p>When it comes to lawful conduct, if a company is ever sued because of the decisions which its Machine Learning models were making, the company is liable for damages unless they are able to understand the inner workings of their models and explain such decisions to the public, assuming the decision making behind those decisions were justifiable.&lt;/p>
&lt;p>In order to comply with regulatory frameworks, companies need to be able to explain their processes in order to get regulatory approval. If a company&amp;rsquo;s Anti Money Laundering (AML) processes come into question caused by a particular case falling through the cracks, &amp;ldquo;Machine Learning&amp;rdquo; as an excuse will not fly. This is why easy to understand and transparent Machine Learning algorithms such as Decision Trees[4] tend to be preferred when applied to compliance problems.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/revolut-aml.png" alt="alt text" title="Revolut AML scandal">
&lt;em>&lt;a href="https://www.telegraph.co.uk/technology/2019/02/28/revolut-failed-block-suspicious-transactions/">telegraph&lt;/a>&lt;/em>&lt;/p>
&lt;h2 id="competitions-do-not-help-you-understand-your-model">Competitions do not help you understand your model&lt;/h2>
&lt;p>The ultimate goal of a competition is to obtain the highest accuracy where even a fraction could make a difference, all under time constrained conditions. This does not encourage you to take the time to understand the data, its imperfections, and train a model which is robust to adversarial examples. Instead you are incentivised to try anything which will edge you forward even slightly, no matter the cost. This encourages the use of black box models, leaving you unable to understand what your model is learning from your data.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/data-science.jpg" alt="alt text" title="Data Scientist hacking things together">&lt;/p>
&lt;p>Competitions tend to use data sets which are less complex in order to lower the barrier of entry and boost participation, apart from the time constraint of course. They also do this because they&amp;rsquo;re not allowed to publicly share personally identifiable information (PII) about their customers. Another reason is that a company&amp;rsquo;s data is propriety. There&amp;rsquo;s a reason why data is called &lt;a href="https://www.forbes.com/sites/forbestechcouncil/2019/11/15/data-is-the-new-oil-and-thats-a-good-thing/">&amp;ldquo;the new oil.&amp;quot;&lt;/a> No company is going to willingly give out their secret sauce for others to use for their own businesses. This limits the things you can learn from competing in such competitions.&lt;/p>
&lt;h2 id="not-all-submissions-require-source-code">Not all submissions require source code&lt;/h2>
&lt;p>If you were not incentivised enough to produce a black box solution to yield the highest accuracy, some competitions do not even enforce the need to submit the code behind your solutions for others to try, placing further importance on the accuracy result rather than the method that was used to achieve said result.&lt;/p>
&lt;h2 id="you-are-not-required-to-write-good-clean-code">You are not required to write good clean code&lt;/h2>
&lt;p>Working as a Data Scientist is not just about modelling, it is also about writing good clean code, in which others are able to understand and reproduce the results which you obtained with the same data. Since Data Science challenges are so focused on model accuracy, you are not being incentivised to properly document your code and the approach you took to reach the conclusions which you presented. This stops others from being able to learn from your methods, and of course stops you from doing the same from others.&lt;/p>
&lt;h2 id="you-are-not-taught-sql">You are not taught SQL&lt;/h2>
&lt;p>SQL is one of if not the most important skill to have in Data Science. Without any knowledge of SQL, you will not be able to query the databases where your data lies in order to get the data you need in the format which your model requires. Sure you can dump a table to a file and then just use &lt;a href="https://pandas.pydata.org/">Pandas&lt;/a> to do the rest, but that will not get you very far.&lt;/p>
&lt;p>What if your database spans in the hundreds of GBs, if not TBs?. The general rule of thumb is that in Pandas you need ten times the amount of RAM that you do data. This means that for just 1GB of data you will need 10GB of RAM. Databases are far more efficient at processing data than a library like Pandas, which is why I always recommend to do as much processing as possible in the database, and only extract the data as a last resort once you really need a library like Pandas to continue with your work. This is not to say that there are not more efficient libraries than Pandas[5], however there is an efficiency v.s complexity trade-off which you will have to make.&lt;/p>
&lt;h2 id="how-to-improve-your-data-science-skills">How to improve your Data Science skills&lt;/h2>
&lt;p>If you are looking for ways to learn, find a particular industry that interests you and try to apply Machine Learning to it. Let&amp;rsquo;s say you are into real estate and would like to train a model to predict the value of a property based on its features. You might even try to forecast the future price increase X years in the future. This could help others looking to buy property and shed a light on the price fluctuations in the property market right now.&lt;/p>
&lt;p>You could definitely find a data set online which will give you this information, however as I explained above these data sets will not hold in real world examples. It would be more realistic if you found a website(s) listing the information you need and build a &lt;a href="https://scrapy.org/">scraper&lt;/a> to collect the data you need and schedule it to run on a regular basis so that you can start to collect historical data over time. In certain cases some websites may even offer an API to make your life a little easier in getting the data which you need.&lt;/p>
&lt;p>Simply scraping the data is not enough, since you are going to do this regularly you need to think of an efficient way to store the data and keep track of the historicals. This is where a database would come in. There is no need to go off and set up a distributed database, &lt;a href="https://www.sqlite.org/index.html">SQLite&lt;/a> would suffice for a project like this, saving you the pain of having to setup and maintain a database server, while still providing you a fast SQL query engine you can use to query your data.&lt;/p>
&lt;p>You can now start to analyse the data you are collecting over time and understand the features that really matter in evaluating the value of a property along with the events which cause the prices to move over time. Once you have a model you might even want to serve it over an API for others to use, allowing them to input the property features and get back a prediction from your model.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/hacker-man.jpg" alt="alt text" title="Hacker man">&lt;/p>
&lt;p>Why am I suggesting all of this? Because although it is easy to just find a data set on the internet, this is not how you are going to get your data in industry. And if you have found your data set online, chances are someone has already done something with it. This might be your chance to try something new.&lt;/p>
&lt;h2 id="what-competitions-are-good-for">What competitions are good for&lt;/h2>
&lt;p>This is not to say that competitions do not have a place at all in Data Science. I do believe that such competitions present an excellent way of bouncing off ideas from each other, assuming the code was submitted, in order for you to try new methods out on your own data.&lt;/p>
&lt;p>Good competitions which also include write ups present a great opportunity for knowledge sharing within the community, allowing for the community to ask questions about the submitter&amp;rsquo;s approach.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/sharing-is-caring.jpg" alt="alt text" title="Sharing is caring">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>[0] Kaggle is a site renowned for bringing Data Scientists and Machine Learning practitioners together to enter competitions to solve Data Science challenges.&lt;/li>
&lt;li>[1] A test set is a subset of the data used to test the accuracy of your model based on data it has never seen before. In the case of competitions test sets tend to have the dependant variable omitted.&lt;/li>
&lt;li>[2] A dependant variable is the value you&amp;rsquo;re trying to predict which is dependant on the values of other variables.&lt;/li>
&lt;li>[3] Overfitting is &amp;ldquo;the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably&amp;rdquo;.&lt;/li>
&lt;li>[4] A decision tree is a diagram or chart that helps determine a course of action or show a statistical probability. Each branch of the decision tree represents a possible decision, outcome, or reaction. The furthest branches on the tree represent the end results of a certain decision pathway.&lt;/li>
&lt;li>[5] &lt;a href="https://numpy.org/">NumPy&lt;/a> is a Python library used to compute mathematical operations on data. It is written in C which makes it blazingly fast compared to other libraries.&lt;/li>
&lt;/ul></content></entry><entry><title type="html">10 Reasons Why You Should Learn Julia</title><link href="https://gabriel.gaucimaistre.com/2018/09/10-reasons-why-you-should-learn-julia/"/><id>https://gabriel.gaucimaistre.com/2018/09/10-reasons-why-you-should-learn-julia/</id><author><name>Gabriel Gauci Maistre</name></author><published>2018-09-27T23:00:00+00:00</published><updated>2018-09-27T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2018/09/10-reasons-why-you-should-learn-julia/">&lt;p>Even though it has 1-based indexing.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/julia-repl.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;h2 id="what-is-julia">What is Julia?&lt;/h2>
&lt;p>Julia is a fairly modern language, developed in 2009 by Jeff Bezanson, &lt;a href="https://en.wikipedia.org/wiki/Stefan_Karpinski">Stefan Karpinski&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Viral_B._Shah">Viral B. Shah&lt;/a>, and &lt;a href="https://en.wikipedia.org/wiki/Alan_Edelman">Alan Edelman&lt;/a> who had the idea of designing a language that was free, high-level, and fast. The language was officially unveiled to the world in 2012 when the team launched a website with a blog post explaining the language’s mission. When asked why they named the language “Julia”, Alan Edelman turned down the thought that it was named after the fractal, but claimed that it just came up in a random conversation years ago when someone suggested arbitrarily that “Julia” would be a good name for a programming language. When asked why they created Julia, they claimed that they came from different programming language backgrounds, and loved them all. But they were greedy, and wanted more.&lt;/p>
&lt;blockquote>
&lt;p>We want a language that’s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that’s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled. — julialang.org&lt;/p>
&lt;/blockquote>
&lt;h2 id="do-we-really-need-another-language">Do we really need another language?&lt;/h2>
&lt;p>Well, yes. Why, you ask?&lt;/p>
&lt;p>Programming languages are used for solving various real world problems, but there is not one language that can solve them all, requiring particular approaches. Some are efficient and have more advantages to use over the others. If we take a look at C for instance, C was the most powerful language there was. Everyone thought C would be the language to end all other languages, but then there came a need for a language which could be used to represent real world problems through Object Oriented Programming (OOP) concepts, which is how C++ was born. So why was Java invented then? People needed something as powerful as C++, but slightly easier to use.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/periodic-table-programming-languages.png" alt="alt text" title="Logo Title Text 1">
&lt;em>Periodic table of programming languages&lt;/em>&lt;/p>
&lt;p>There will never be the perfect language. Programming languages will never be a finished product, as the world evolves and specific needs change, so will programming languages.&lt;/p>
&lt;h2 id="here-are-10-reasons-why">Here are 10 reasons why&lt;/h2>
&lt;h3 id="1its-blazingly-fast-right-out-of-the-box">1 — It’s blazingly fast right out of the box&lt;/h3>
&lt;p>Julia was designed from the beginning with high performance in mind without having to sacrifice ease of use like garbage collection, which is a common trade off in languages such as C++. Julia applications can compile to efficient native code for multiple platforms thanks to the &lt;a href="https://llvm.org/">LLVM compiler&lt;/a>.&lt;/p>
&lt;p>Compilers can be extremely efficient when you tell them exactly what to do with your code. Take C for instance, a language which requires you to explicitly define the types of your variables and what you plan on doing with them, such as performing a simple addition. Since the CPU has dedicated hardware for addition arithmetic, it knows exactly what to do and does it extremely well and quickly.&lt;/p>
&lt;p>Now take an interpreted language like Python for instance. You give the interpreter two variables to add together, but the CPU has no concept of variables. This means that the CPU must wait for instructions from the interpreter to figure out what these two variables contain. Once that has been figured out, the next step would be to select the right operation. If they are integers, perform integer addition, if they are floats, perform floating point addition, if they are integer and float, convert the integer to a float, then perform a floating point addition. The interpreter has to go through this process every single time an operation is called on a variable. This is what makes Python so slow compared to languages such as C.&lt;/p>
&lt;p>Now let’s focus on Julia. If C and Python are on other ends of the spectrum, Julia sits right in the middle. Julia’s compiler doesn’t have to know beforehand what type of variable you’re trying to use, but it cleverly plans ahead whenever you call a function. When a function is called in Julia, the arguments are already known. Julia’s compiler uses this knowledge to figure out the exact CPU instructions necessary for the particular arguments by peeking into the function. Once the exact instructions are mapped out, Julia can execute them very quickly. This is why calling a function in Julia takes long the first time. During this period of time, Julia’s compiler would be figuring out all the types of variables being used and compiles them all into fast and precise CPU instructions. This means that when the same function is called repeatedly, consequent calls run much, much faster.&lt;/p>
&lt;h3 id="2it-solves-the-two-language-problem">2 — It solves the two language problem&lt;/h3>
&lt;p>The industry is currently plagued by what’s called the “two-language problem”. Typically, developers first prototype their application using a slow dynamic language, and then rewrite it using a fast static language for production.&lt;/p>
&lt;blockquote>
&lt;p>We set out to solve the “two language problem” back in 2009. Much of our progress in parallel computing was thwarted by the fact that while the users are programming in a high-level language such as R and Python, the performance-critical parts have to be rewritten in C/C++ for performance. This is hugely inefficient, because it introduces human error and wasted effort, slows time to market and allows competitors to leapfrog ahead. This two language problem hinders not just researchers, but also quants, scientists, data scientists and engineers in the industry. — Viral Shah, co-founder of Julia Computing&lt;/p>
&lt;/blockquote>
&lt;p>In fact, many companies have toyed around with in-house languages trying to do what Julia does for this very reason, but having an open-source language built on best-of-breed compiler technology is far better.&lt;/p>
&lt;h3 id="3it-excels-at-technical-computing">3 — It excels at technical computing&lt;/h3>
&lt;p>Designed with data science in mind, Julia excels at numerical computing with a syntax that is great for math, with support for many numeric data types, and providing parallelism out of the box, but more on that last bit later. Julia’s multiple dispatch is a natural fit for defining number and array-like data types.&lt;/p>
&lt;p>The Julia REPL (Read/Evaluate/Print/Loop) &lt;a href="https://docs.julialang.org/en/v1/manual/unicode-input/#Unicode-Input-1">provides easy access to special characters&lt;/a>, such as Greek alphabetic characters, subscripts, and special maths symbols. If you type a backslash, you can then type a string (usually the equivalent LATEX string) to insert the corresponding character. This is great as it allows developers to just derive some equation and directly type it in. For example, if you type:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">julia&lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#f92672">\&lt;/span>sqrt &lt;span style="color:#f92672">&amp;lt;&lt;/span>TAB&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Julia replaces the \sqrt with a square root symbol:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">julia&lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#f92672">√&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Some other examples:&lt;/p>
&lt;ul>
&lt;li>\Gamma Γ&lt;/li>
&lt;li>\mercury ☿&lt;/li>
&lt;li>\degree °&lt;/li>
&lt;li>\cdot ⋅&lt;/li>
&lt;li>\in ∈&lt;/li>
&lt;/ul>
&lt;h3 id="4its-composable">4 — It’s composable&lt;/h3>
&lt;p>Julia packages naturally work well together. This is thanks to the language’s function composition which makes it easy to pass two or more functions as arguments. Julia has a dedicated function composition operator (∘) for achieving this.&lt;/p>
&lt;p>For example, the sqrt() and + functions can be composed like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">julia&lt;span style="color:#f92672">&amp;gt;&lt;/span> (sqrt &lt;span style="color:#f92672">∘&lt;/span> &lt;span style="color:#f92672">+&lt;/span>)(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>)
(sqrt &lt;span style="color:#f92672">∘&lt;/span> &lt;span style="color:#f92672">+&lt;/span>)(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">5&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>which adds the numbers first, then finds the square root.&lt;/p>
&lt;p>This example composes three functions.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">julia&lt;span style="color:#f92672">&amp;gt;&lt;/span> map(first &lt;span style="color:#f92672">∘&lt;/span> reverse &lt;span style="color:#f92672">∘&lt;/span> uppercase, split(&lt;span style="color:#e6db74">&amp;#34;you can compose functions like this&amp;#34;&lt;/span>))
&lt;span style="color:#ae81ff">6&lt;/span>&lt;span style="color:#f92672">-&lt;/span>element &lt;span style="color:#66d9ef">Array&lt;/span>{&lt;span style="color:#66d9ef">Char&lt;/span>,&lt;span style="color:#ae81ff">1&lt;/span>}&lt;span style="color:#f92672">:&lt;/span>
&lt;span style="color:#e6db74">&amp;#39;U&amp;#39;&lt;/span>
&lt;span style="color:#e6db74">&amp;#39;N&amp;#39;&lt;/span>
&lt;span style="color:#e6db74">&amp;#39;E&amp;#39;&lt;/span>
&lt;span style="color:#e6db74">&amp;#39;S&amp;#39;&lt;/span>
&lt;span style="color:#e6db74">&amp;#39;E&amp;#39;&lt;/span>
&lt;span style="color:#e6db74">&amp;#39;S&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This was just a basic example, but Julia makes it easy for packages to communicate with each other. Matrices of unit quantities, or data table columns of currencies and colours, just work — and with good performance.&lt;/p>
&lt;h3 id="5it-does-parallel-really-well">5 — It does parallel really well&lt;/h3>
&lt;p>Data has gotten so huge that it has become unpractical to run applications on a single computer. Nowadays everyone is computing big data on multiple nodes in a cluster in order to decrease the execution time by running tasks in parallel, but unfortunately many languages were never designed for this.&lt;/p>
&lt;p>Take Python for instance, its interpreter does not do parallel very well as it was designed with the primary assumption that an individual Python script is serial having a single thread of execution. The interpreter makes use of what’s called a Global Interpreter Lock (GIL) which ensures that only a single line of a Python script can be interpreted at a time, thereby preventing memory corruption caused by multiple threads trying to read, write, or delete memory in parallel.&lt;/p>
&lt;p>There are of course ways around this issue, such as using the &lt;a href="http://spark.apache.org/">Spark&lt;/a> Python bindings &lt;a href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.html">PySpark&lt;/a>, however not as straightforward as one would hope. PySpark will not run your Python code, but instead use a package called &lt;a href="https://www.py4j.org/">Py4J&lt;/a> which will enable Spark to communicate between the Python interpreter and the &lt;a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java Virtual Machine (JVM)&lt;/a>. This is because Spark is built on top of &lt;a href="https://www.scala-lang.org/">Scala&lt;/a> which runs on the JVM. This creates a layer of abstraction between your code and the execution of that code. One of the main issues with this is that although you would write Python code, Spark can return errors in Java on code which you would have never written.&lt;/p>
&lt;p>Julia was designed for parallelism from the ground-up, and provides built-in primitives for parallel computing at every level: &lt;a href="https://docs.julialang.org/en/stable/base/simd-types/">instruction level parallelism&lt;/a>, multi-threading, and distributed computing. The Celeste.jl project &lt;a href="https://arxiv.org/pdf/1801.10277.pdf">achieved 1.5 PetaFLOP/s&lt;/a> on the &lt;a href="https://cs.lbl.gov/news-media/news/2016/celeste-enhancements-create-new-opportunities-in-sky-surveys/">Cori supercomputer at NERSC&lt;/a> using 650,000 cores.&lt;/p>
&lt;p>The Julia compiler can also generate native code for various hardware accelerators, such as &lt;a href="https://github.com/JuliaGPU/CUDAnative.jl">GPUs&lt;/a> and Xeon Phis. Packages such as &lt;a href="https://github.com/JuliaParallel/DistributedArrays.jl">DistributedArrays.jl&lt;/a> and &lt;a href="https://github.com/JuliaParallel/Dagger.jl">Dagger.jl&lt;/a> provide higher levels of abstraction for parallelism.&lt;/p>
&lt;h3 id="6its-codebase-is-written-entirely-in-julia">6 — Its codebase is written entirely in Julia&lt;/h3>
&lt;p>That’s right. This is a feature most significant compared to typical dynamic languages. If you can develop applications in Julia, then you can also contribute to Julia. If you need to peek underneath the hood and see what’s up, you’re not going to find C code as you would with Python.&lt;/p>
&lt;p>Due to being an interpreted language, Python can be extremely slow compared to older languages &lt;a href="http://www.infoworld.com/article/2608770/application-development/application-development-stroustrup-why-the-35-year-old-c-still-dominates-real-dev.html">such as C/C++&lt;/a> and even newer ones &lt;a href="http://www.infoworld.com/article/2877924/application-development/go-rust-road-ahead-young-programming-languages.html">such as Go&lt;/a>. However over the years, many Python packages have been optimised to execute at C speed. This might sound great at first, but not when you consider the trade-offs that were taken to achieve this.&lt;/p>
&lt;p>Python allows developers to add C extensions. This is great for when you have calculations which could happen several times, allowing further optimisations to be made since such C extensions could be compiled and thus run much faster. Developers can write their code in C++ and call it from within their Python code, giving them a huge performance bump.&lt;/p>
&lt;p>However this takes away from the beauty of Python. Python is supposed to be a simple to use, easy to read language. Adding C/C++ to the mixture ruins that, so might as well just use C/C++ in the first place.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/numpy-cpp.png" alt="alt text" title="Logo Title Text 1">
&lt;em>Numpy is written more in C/C++ than it is in Python&lt;/em>&lt;/p>
&lt;p>Julia solves this problem however, since it can be as fast as C/C++, without having the need to make such trade-offs. The core language imposes very little; Julia Base and the standard library is written in Julia itself, including primitive operations like integer arithmetic.&lt;/p>
&lt;h3 id="7good-call-support-to-other-languages">7 — Good call support to other languages&lt;/h3>
&lt;p>Although most things can be done in Julia, if you wish to make use of libraries which have already been written in C and Fortran, Julia makes it easy to do so in a simple and efficient way. Julia was designed with a “no boilerplate” philosophy in mind where functions can be called directly from Julia without any “glue” code, code generation, or compilation, even from the interactive prompt. This is done using Julia’s &lt;a href="https://docs.julialang.org/en/v1/base/c/#ccall">ccall&lt;/a> syntax, which looks like an ordinary function call.&lt;/p>
&lt;p>Julia’s external call support does not stop here. Julia can also interface with &lt;a href="https://github.com/Keno/Cxx.jl">C++&lt;/a>, &lt;a href="https://github.com/JuliaPy/PyCall.jl">Python&lt;/a>, &lt;a href="https://github.com/JuliaInterop/RCall.jl">R&lt;/a>, &lt;a href="https://github.com/JuliaInterop/JavaCall.jl">Java&lt;/a>, and many other languages, making it possible to even share data between the two languages. Julia can also be embedded in other programs through its &lt;a href="https://docs.julialang.org/en/stable/manual/embedding/">embedding API&lt;/a>. Specifically, Python applications can call Julia using &lt;a href="https://github.com/JuliaPy/pyjulia">PyJulia&lt;/a>. R programs can do the same with R’s &lt;a href="https://cran.r-project.org/web/packages/JuliaCall/index.html">JuliaCall&lt;/a>, which is demonstrated by &lt;a href="https://rpubs.com/dmbates/377897">calling MixedModels.jl from R&lt;/a>.&lt;/p>
&lt;h3 id="8its-dynamic-and-easy-to-understand">8 — It’s dynamic and easy to understand&lt;/h3>
&lt;p>There exist two types of programming languages, static languages where you’re required to have a type computable before the execution of the program, and dynamic languages where nothing is known about types until run time, when the actual values manipulated by the program are available.&lt;/p>
&lt;p>Julia’s type system is dynamic, but gains some of the advantages of static type systems by making it possible to indicate that certain values are of specific types. This can be of great assistance in generating efficient code, but even more significantly, it allows method dispatch on the types of function arguments to be deeply integrated with the language.&lt;/p>
&lt;p>The default behaviour in Julia when types are omitted is to allow values to be of any type. Thus, one can write many useful Julia functions without ever explicitly using types. When additional expressiveness is needed, however, it is easy to gradually introduce explicit type annotations into previously “untyped” code. Adding annotations serves three primary purposes: to take advantage of Julia’s powerful multiple-dispatch mechanism, to improve human readability, and to catch programmer errors.&lt;/p>
&lt;h3 id="9its-optionally-typed">9 — It’s optionally typed&lt;/h3>
&lt;p>Julia has a rich language of descriptive data types, and type declarations can be used to clarify and solidify applications.&lt;/p>
&lt;p>You can define functions with optional arguments, so that the function can use sensible defaults if specific values aren’t supplied. You provide a default symbol and value in the argument list:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">julia&lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">function&lt;/span> xyzpos(x, y, z&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>)
println(&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">$x&lt;/span>&lt;span style="color:#e6db74">, &lt;/span>&lt;span style="color:#e6db74">$y&lt;/span>&lt;span style="color:#e6db74">, &lt;/span>&lt;span style="color:#e6db74">$z&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
&lt;span style="color:#66d9ef">end&lt;/span>
xyzpos (generic &lt;span style="color:#66d9ef">function&lt;/span> with &lt;span style="color:#ae81ff">2&lt;/span> methods)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And when you call this function, if you don’t provide a third value, the variable &lt;code>z&lt;/code> defaults to 0 and uses that value inside the function.&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-julia" data-lang="julia">julia&lt;span style="color:#f92672">&amp;gt;&lt;/span> xyzpos(&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>)
&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>
julia&lt;span style="color:#f92672">&amp;gt;&lt;/span> xyzpos(&lt;span style="color:#ae81ff">1&lt;/span>,&lt;span style="color:#ae81ff">2&lt;/span>,&lt;span style="color:#ae81ff">3&lt;/span>)
&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="10it-can-do-general-purpose-programming">10 — It can do general-purpose programming&lt;/h3>
&lt;p>Although Julia is designed as a technical language first, this does not mean that it can’t be used for other things. Just like Python, Julia can also be used for writing software in the widest variety of application domains. This is because Julia does not include language constructs designed to be used within a specific application domain. Julia lets you &lt;a href="https://github.com/JuliaGizmos/WebIO.jl">write UIs&lt;/a>, &lt;a href="https://github.com/JuliaLang/PackageCompiler.jl">statically compile&lt;/a> your code, or even deploy it on a &lt;a href="https://github.com/JuliaWeb/HTTP.jl">webserver&lt;/a>. It also has powerful shell-like capabilities for &lt;a href="https://docs.julialang.org/en/stable/manual/running-external-programs/">managing other processes&lt;/a>. It provides Lisp-like macros and other &lt;a href="https://docs.julialang.org/en/stable/manual/metaprogramming/">metaprogramming&lt;/a> facilities. The standard library also provides asynchronous I/O, process control, logging, profiling, and more.&lt;/p>
&lt;p>Julia uses multiple dispatch as a paradigm, making it easy to express many object-oriented and functional programming patterns. This allows developers to change the behaviour of functions based on the run-time’s state of more than one of its arguments. This is similar to single-dispatch where a function or method call is dynamically dispatched based on the actual derived type of the object on which the method has been called. Multiple dispatch takes it a step further by routing the dynamic dispatch to the implementing function or method using the combined characteristics of one or more arguments.&lt;/p>
&lt;p>Julia also ships with an amazing package manager which is designed around “environments” instead of a single global set of packages like traditional package managers. In Julia, packages can either be local to an individual project, or shared and selected by name. Environments are maintained in a manifest file, containing the exact set of packages and versions which a particular application needs. If you’ve ever tried to run code you haven’t used in a while only to find that you can’t get anything to work because you’ve updated or uninstalled some of the packages your project was using, you’ll understand why such an approach is needed. Thanks to such environments, each application maintains its own its own independent set of package versions. This greatly improves reproducibility, allowing developers to check out a project on a new system, simply materialize the environment described by its manifest file, and immediately be up and running with a known-good set of dependencies.&lt;/p>
&lt;h3 id="bonusits-reached-10">Bonus — It’s reached 1.0&lt;/h3>
&lt;p>This is great because this means that the language has reached a state of “fully baked”, and the Julia core contributors have pledged a commitment to language API stability which means that code written to run for Julia 1.0 will continue to work in Julia 1.1, 1.2, etc. This means that the core language developers and community alike can focus on packages, tools, and new features built upon this solid foundation.&lt;/p>
&lt;h2 id="where-to-go-from-here">Where to go from here&lt;/h2>
&lt;p>JuliaBox lets developers prototype their Julia code in &lt;a href="http://jupyter.org/">Jupyter notebooks&lt;/a> right in their browser. JuliaBox is the number one choice for universities teaching Julia since students can start using Julia in seconds with no installation required.&lt;/p>
&lt;blockquote>
&lt;p>JuliaBox has been running continuously since January 2015 at &lt;a href="http://www.juliabox.com">www.juliabox.com&lt;/a> and has delighted over 50,000 users. It provides the popular Jupyter notebook interface to run Julia. Many common Julia packages, such as those included in JuliaPro are available by default and other packages can be installed by users. JuliaBox is extremely popular for teaching Julia. It is available free for limited usage today. — juliabox.com&lt;/p>
&lt;/blockquote>
&lt;p>JuliaBox includes a number of introductory tutorials to get you up and running with the language, but also go a step further and include advanced tutorials to get you up to speed with the language’s capabilities.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/julia-box.png" alt="alt text" title="Logo Title Text 1">
&lt;em>Getting started tutorials on JuliaBox&lt;/em>&lt;/p></content></entry><entry><title type="html">Are You Ready For GDPR?</title><link href="https://gabriel.gaucimaistre.com/2018/02/are-you-ready-for-gdpr/"/><id>https://gabriel.gaucimaistre.com/2018/02/are-you-ready-for-gdpr/</id><author><name>Gabriel Gauci Maistre</name></author><published>2018-02-10T23:00:00+00:00</published><updated>2018-02-10T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2018/02/are-you-ready-for-gdpr/">&lt;p>The General Data Protection Regulation (GDPR) will come into effect on the 25th of May, 2018; along with it a number of penalties for companies who fail to abide by these rules. These penalties can be very harsh, up to €20M or 4% of your company’s annual worldwide revenue. The European Commission (EC) has stated that no exceptions will be made after the 25th of May, and failure to comply will result in hefty fines. Despite these harsh penalties, not all companies are taking this new regulation seriously who will no doubt find themselves in a world of panic just before GDPR hits. Many more fail to understand the point of this new regulation and what it swears to protect. In light of this, I thought I’d give a brief overview of this new regulation and what it entails.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/you-shall-not-violate-my-privacy.jpg" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;h1 id="what-is-gdpr">What is GDPR?&lt;/h1>
&lt;p>GDPR is a set of rules laid out by the EC which safeguards the consumer’s right to privacy. It not only applies to all EU companies, but even those outside of the EU having EU based customers. So that affects most companies, which is a good thing. The point of this regulation is to simply put into legislation a lot of common sense data security principles such as the reduction and deletion of unnecessary personal data collection, restricting access, and securing data. It also addresses the export of personal data outside the EU. Unfortunately not everyone agrees with these security principles, as some are choosing to opt out of EU markets just to escape this regulation.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/gdpr-stopped-selling-stuff-europe.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>GDPR is a set of rules laid out by the EC which safeguards the consumer’s right to privacy. It not only applies to all EU companies, but even those outside of the EU having EU based customers. So that affects most companies, which is a good thing. The point of this regulation is to simply put into legislation a lot of common sense data security principles such as the reduction and deletion of unnecessary personal data collection, restricting access, and securing data. It also addresses the export of personal data outside the EU. Unfortunately not everyone agrees with these security principles, as some are choosing to opt out of EU markets just to escape this regulation.&lt;/p>
&lt;h1 id="why-do-we-need-it">Why do we need it?&lt;/h1>
&lt;p>It’s simple, our current laws are outdated. The EU Data Protection Directive (DPD) was adopted in 1995, just over 22 years ago. That’s only 3 years after Dial-up was introduced. I think we can all agree that many things have changed since then. The way we process data in 2018 is completely different to the way it was being done in 1995, so our laws need to be updated to reflect these changes. The GDPR was created to fill those massive gaps left by the DPD. Companies are collecting more data about consumers every year, some of which highly sensitive, couple that with carelessness and that can become very dangerous.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/equifax-hack-143-million.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>I’m sure you remember all the controversy surrounding Equifax and their indiscretions last year. A consumer credit reporting agency which failed to secure the information of over 800 million individual consumers and more than 88 million businesses worldwide.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/katie-van-fleet-equifax-stolen-identity-us.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/your-employer-may-share-your-salary-equifax-might-sell.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>Last September, Equifax announced a cybercrime identity theft event potentially impacting approximately 145.5 million U.S. consumers. Information on an estimated range of under 400,000 up to 44 million British residents as well as 8,000 Canadian residents were also compromised.&lt;/p>
&lt;pre>&lt;code>“In today’s information economy, data is an enormous asset, but if companies like Equifax can’t properly safeguard the enormous amounts of highly sensitive data they are collecting and centralising, then they shouldn’t be collecting it in the first place.” — Mark Warner on the Equifax data breach
&lt;/code>&lt;/pre>
&lt;p>This is why we need GDPR. These companies must be held accountable for their carelessness when handling with consumer data.&lt;/p>
&lt;h1 id="i-need-some-definitions">I need some definitions&lt;/h1>
&lt;p>The EC defines a &lt;strong>data controller&lt;/strong> as someone who determines the purposes and means of the processing of personal data, while a &lt;strong>data processor&lt;/strong> is someone who processes personal data on behalf of the controller. This means the company collecting consumer data is labelled the “data controller”, while the 3rd party processing the data on behalf of the company is the “data processor”.&lt;/p>
&lt;p>&lt;strong>Processing&lt;/strong> is defined as an act performed on all or sets of personal data, whether through automated means or not, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.&lt;/p>
&lt;p>&lt;strong>Personal data&lt;/strong> is defined as any information relating to an identified or identifiable natural person (‘data subject’); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.&lt;/p>
&lt;p>A &lt;strong>personal data breach&lt;/strong> is defined as a security breach resulting in the accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to, personal data transmitted, stored or otherwise processed.&lt;/p>
&lt;h1 id="how-do-we-comply">How do we comply?&lt;/h1>
&lt;p>The only exception the EC makes is maintaining a record of processing activities under a company’s responsibility, and maintaining a record of all categories of processing activities carried out on behalf of a controller. This exception applies to enterprises or organisations employing fewer than 250 persons unless the processing it carries out is likely to result in a risk to the rights and freedoms of data subjects, the processing is not occasional, or the processing includes special categories of data or personal data relating to criminal convictions and offences.&lt;/p>
&lt;p>This means that all other articles enforced by the GDPR apply to everyone, so I thought I’d list a few important requirements to consider.&lt;/p>
&lt;h2 id="art-15right-of-access-by-the-data-subjecthttpsgdpr-infoeuart-15-gdpr">&lt;a href="https://gdpr-info.eu/art-15-gdpr/">Art. 15 — Right of access by the data subject&lt;/a>&lt;/h2>
&lt;p>This article hands over the right to the data subject, the “consumer”, to request and obtain the confirmation of any personal data being processed concerning the individual. If personal data is being processed, the controller must also voice the purposes behind the processing of the data and document the recipients to whom the personal data has or will be disclosed to, in particular those located outside the EU. Data subjects also have the right to know how long the controller wishes to store their personal data for and the reasoning behind it. Data controllers must also provide the possibility for the data subject to request the rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject. If such rights are not granted, data subjects must have the right to lodge such complaints to a supervisory authority representing the company, the Data Protection Officer (DPO). If the existence of automated decision making is present, including profiling, the data subject must be made aware of this.&lt;/p>
&lt;p>If personal data is transferred to a location outside of the EU, the data subject must be informed of the appropriate safeguards being taken in relation to the transfer.&lt;/p>
&lt;p>All these rights granted to the data subject should be presented in the form of a privacy policy, preferably online in an easy to understand accessible format. Tesco’s privacy policy is a leading example in the right of access for the data subject&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/tesco-privacy-policy.png" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;h2 id="art-16right-to-rectificationhttpsgdpr-infoeuart-16-gdpr">&lt;a href="https://gdpr-info.eu/art-16-gdpr/">Art. 16 — Right to rectification&lt;/a>&lt;/h2>
&lt;p>After the data subject’s request to their personal data is processed, if their data is found to be incorrect, they shall have the right to obtain from the controller without undue delay the rectification of inaccurate personal data concerning them.&lt;/p>
&lt;h2 id="art-17right-to-be-forgottenhttpsgdpr-infoeuart-17-gdpr">&lt;a href="https://gdpr-info.eu/art-17-gdpr/">Art. 17 — Right to be forgotten&lt;/a>&lt;/h2>
&lt;p>The data subject may also choose to request the erasure of personal data concerning them without undue delay from the controller. It is the data controller’s responsibility to inform 3rd party controllers which are processing the personal data that the data subject has requested the erasure. An exception to this is made if the personal data is still necessary in relation to the purposes for which they were collected or processed.&lt;/p>
&lt;p>The EC defines “necessary” processing as processing being made to either exercise the right of freedom of expression and information and compliance with a legal obligation.&lt;/p>
&lt;h2 id="art-18right-to-restriction-of-processinghttpsgdpr-infoeuart-18-gdpr">&lt;a href="https://gdpr-info.eu/art-18-gdpr/">Art. 18 — Right to restriction of processing&lt;/a>&lt;/h2>
&lt;p>If the data subject finds that the accuracy of their personal data is incorrect, the purposes for processing are no longer relevant, or that the processing is deemed unlawful, then the data subject has the right to request the restriction of processing. This entails that such personal data shall, with the exception of storage, only be processed with the data subject’s consent or for the establishment, exercise or defence of legal claims or for the protection of the rights of another natural or legal person or for reasons of important public interest of the Union or of a Member State.&lt;/p>
&lt;h2 id="art-20right-to-data-portabilityhttpsgdpr-infoeuart-20-gdpr">&lt;a href="https://gdpr-info.eu/art-20-gdpr/">Art. 20 — Right to data portability&lt;/a>&lt;/h2>
&lt;p>This article lays out the data subject’s right to request and obtain from the controller the data concerning them which has been provided to the controller. This data is to be provided in a commonly used electronic form, and reserves the right to transmit such data to another controller without hindrance from the controller to which the personal data have been provided.&lt;/p>
&lt;p>Data subjects who have been granted the restriction of processing must be informed if their restriction is to be lifted beforehand.&lt;/p>
&lt;h2 id="art-21right-to-objecthttpsgdpr-infoeuart-21-gdpr">&lt;a href="https://gdpr-info.eu/art-21-gdpr/">Art. 21 — Right to object&lt;/a>&lt;/h2>
&lt;p>The data subject reserves the right to object, on grounds relating to his or her particular situation, at any time in the course of the processing of their personal data. The controller shall no longer process the personal data unless the controller demonstrates compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject or for the establishment, exercise or defence of legal claims.&lt;/p>
&lt;h2 id="art-22right-to-withdraw-consent-to-automated-decision-makinghttpsgdpr-infoeuart-22-gdpr">&lt;a href="https://gdpr-info.eu/art-22-gdpr/">Art. 22 — Right to withdraw consent to automated decision making&lt;/a>&lt;/h2>
&lt;p>Article 22 grants data subjects the right to to withdraw consent, on grounds relating to his or her particular situation, to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her. An exception to this is made if this is necessary for entering into, or performance of, a contract between the data subject and a data controller, or is authorised by Union or Member State law.&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>The GDPR simply calls that you make conscious business decisions when processing your consumer’s personal data. It may feel rather frightening at first, however once you come to understand its importance and follow the advice laid out by the regulation, then you need not worry about the heavy fines set in place. Such fines, although not to be taken lightly, are for extreme cases such as Equifax. Your local data protection regulators will most likely have some checklists into which you’d have to somehow fit, but if you follow best practices, that shouldn’t be an issue.&lt;/p>
&lt;blockquote>
&lt;p>Historically, privacy was almost implicit, because it was hard to find and gather information. But in the digital world, whether it’s digital cameras or satellites or just what you click on, we need to have more explicit rules — not just for governments but for private companies. — Bill Gates&lt;/p>
&lt;/blockquote></content></entry><entry><title type="html">Blocking Social Media at Work</title><link href="https://gabriel.gaucimaistre.com/2018/01/blocking-social-media-at-work/"/><id>https://gabriel.gaucimaistre.com/2018/01/blocking-social-media-at-work/</id><author><name>Gabriel Gauci Maistre</name></author><published>2018-01-22T23:00:00+00:00</published><updated>2018-01-22T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2018/01/blocking-social-media-at-work/">&lt;p>Are you tired of your employees accessing social networks at work? Maybe you feel that your employees are spending more time checking their Facbook feed rather than crunching the numbers. Whether it’s Facebook, Twitter, or YouTube, how dare they do something else other than what you’re paying them for? In fact, why don’t you just strip out all the applications from their computers, lock them down, which let’s face it they probably already are, and leave Excel installed and call it ExcelOS, &lt;em>patent pending&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/how-about-no.jpeg" alt="alt text" title="Logo Title Text 1">&lt;/p>
&lt;p>Okay that’s enough. If you thought the above scenario was a good idea then you should take a minute to read this piece. Blocking social networking could actually cause more harm than good, resulting in a backlash at times. Let’s go through a couple of reasons as to why this is a bad idea.&lt;/p>
&lt;h1 id="why-you-should-not-block-social-netoworks">Why you should not block social netoworks&lt;/h1>
&lt;p>Let’s face it, censorship is not cool. And who would want to work for a company that isn’t cool? Morale is very important for a company and its success. It’s what describes the overall outlook, attitude, satisfaction, and confidence that employees feel at work. With a positive environment at work, employees believe that they can meet their most important career and vocational needs at work. Take that away, and suddenly you’re at risk of attrition caused by contempt. If you want your employees to stay loyal to the company, consider allowing access to social networks.&lt;/p>
&lt;p>Word of mouth is the best form of free advertising nowadays thanks to social networks. It’s something that’s very hard to obtain, yet so easy to lose. Granting access to social networks could give the company a good reputation and keep your employees happy. Take that away and you risk disgruntled employees complaining about your access restriction methods to their peers.&lt;/p>
&lt;blockquote>
&lt;p>“If you make customers unhappy in the physical world, they might each tell 6 friends. If you make customers unhappy on the Internet, they can each tell 6,000 friends.” — Jeff Bezos, CEO at Amazon.com&lt;/p>
&lt;/blockquote>
&lt;p>Would you rather your employees become savvy with the current trends in social networking, or stay stuck in the past? Every year social networking is being further embedded into our lives. Your employees may not be using social networks for work today, but they might need to tomorrow. Allowing your employees access to social networks ensures that they will be ready for this change in trend.&lt;/p>
&lt;p>By cutting off access to social networks, you’re cutting off your employee’s access to the outside world where most communication happens. What if I told you that your employees may be using social networks to post about the company, maybe even job posts to attract a candidate? Would you still want to restrict their access?&lt;/p>
&lt;p>Social networks may also be used as a means to seek help. Many people navigate to Twitter for help on technical questions, possibly even directed towards the authority in charge. YouTube could also be used to look up tutorials on performing certain Excel functions. By blocking access to social networks, you could be restricting your employees from being able to ask for help online when they’re stuck.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/work-site.gif" alt="alt text" title="Logo Title Text 1">
&lt;em>&lt;a href="http://joyreactor.com/post/876142">source&lt;/a>&lt;/em>&lt;/p>
&lt;p>You could also be inarticulately blocking a website your employees use to do their job. Let’s face it, no one expects you to understand all the websites which your employees use. But if that’s the case, then maybe you shouldn’t be the one deciding what websites your employees can and can’t use.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/reddit-it-block.png" alt="alt text" title="Logo Title Text 1">
&lt;em>&lt;a href="https://www.reddit.com/r/webdev/comments/7pxeqn/im_a_freelance_web_dev_and_my_current_clients_it/">source: /r/webdev&lt;/a>&lt;/em>&lt;/p>
&lt;p>Social networks could also help your employees network with employees from other companies. How else are your employees going to know what your competitors are up to? Your employees could also bring forward new ideas and possibly improvements to the company’s benefits.&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Let’s face it. There’s a way around to everything. If your employees really want to access social networks, they will. Whether it’s on their laptop or their phone, they’ll find a way. Should you really be measuring the productivity of your employees from the time they spend working on work related tasks? Isn’t it more important that your employees are getting the work done? So what if they like to reward themselves between tasks by checking their Facebook feed? Having such a system in place could actually keep your employees more productive.&lt;/p>
&lt;p>This doesn’t mean that there won’t be employees who will waste time. Such employees have always existed, long before computers and social networks were ever introduced in the workplace. Simply taking away access to social networks isn’t going to magically make a time wasting employee productive. It’s the management’s job to handle those employee’s whose social networking use is affecting their productivity.&lt;/p>
&lt;p>How often do your employees work beyond their eight hour shift? Take a minute to think about it. Do they check their work emails as soon as they get up? That’s a work related activity at home. Many employees review reports while they’re out, take calls while they’re on vacation, and draft reports before bed. With all this in mind, do you still think it’s fair to block their access?&lt;/p></content></entry><entry><title type="html">The Current State of Job Requirements</title><link href="https://gabriel.gaucimaistre.com/2018/01/the-current-state-of-job-requirements/"/><id>https://gabriel.gaucimaistre.com/2018/01/the-current-state-of-job-requirements/</id><author><name>Gabriel Gauci Maistre</name></author><published>2018-01-11T23:00:00+00:00</published><updated>2018-01-11T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/2018/01/the-current-state-of-job-requirements/">&lt;p>Have you ever looked at a job ad, skimmed through it, and decided not to apply because you thought that you could not even come close to matching the requirements the ad was asking for? Maybe you were job hunting and felt like you were being flooded with job ads having impossible requirements making you shake your head in disbelief.&lt;/p>
&lt;blockquote>
&lt;p>Looking for a Big Data Engineer with an M.Sc or Ph.D degree in a related field, having at least 7 years of experience in Hadoop and Spark. Must also have a few years of experience with AI-powered Blockchain technologies and willing to start yesterday.&lt;/p>
&lt;/blockquote>
&lt;p>Does that sound familiar? Welcome to the current state of job ads; where candidates complain that requirements are too hard to match, and hiring managers claim that they cannot find the candidates they’re looking for. How did we reach this point? Let’s break down the job ad above.&lt;/p>
&lt;blockquote>
&lt;p>Looking for a Big Data Engineer with an M.Sc or Ph.D degree in a related field&lt;/p>
&lt;/blockquote>
&lt;p>Is requiring an M.Sc or Ph.D really justified to work as a big data engineer? If you’ve noticed that a lot of new job descriptions nowadays are asking for a higher education than a graduate degree, then you are not wrong. Masters degrees have now become the new bachelors degrees in the hiring world. If the job has not changed, then why should the candidate’s education? Many hiring managers are now asking for post graduate degrees for the same jobs which only used to require a graduate degree, pushing students further into debt with their student loans, assuming they afford to do so.&lt;/p>
&lt;blockquote>
&lt;p>Having at least 7 years of experience in Hadoop and Spark&lt;/p>
&lt;/blockquote>
&lt;p>How can a candidate possibly fill the job of a big data engineer, requiring 7 years of experience in Hadoop and Spark, when these technologies have not even been out for that long? Although this example is strictly relevant towards IT jobs, these unrealistic expectations are also found in other industries such as engineering in which hiring managers ask for prior experience with specific brands of machine tools. Although it is understandable to want experienced candidates, such unrealistic expectations only make it harder to fill positions in a company.&lt;/p>
&lt;p>&lt;img src="https://gabriel.gaucimaistre.com/images/work-experience.jpeg" alt="alt text" title="Logo Title Text 1">
&lt;em>&lt;a href="https://images.wizbii.com/file/v1/0b7mpab0o6pzyg2bpbtrpbw2pparr52b.jpeg">source&lt;/a>&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Must also have a few years of experience with AI-powered Blockchain technologies and willing to start yesterday&lt;/p>
&lt;/blockquote>
&lt;p>Do I really need to explain this one? Okay, I may have gone little overboard with last part of the job ad, but I just can’t count the number of times I saw something like this in a job description. How many times have you looked at a job description and saw a requirement listed which had nothing to do with the job itself?&lt;/p>
&lt;h1 id="what-is-happening">What is happening?&lt;/h1>
&lt;p>Well, a number of things really. Such job ads are possibly being put together by a number of people all looking for different things in a candidate. This is the number one factor causing job ads to become bloated, possibly no longer relevant to the job itself, and unrealistically requiring more than one candidate from varying professions to fill the same job.&lt;/p>
&lt;p>The recruiter writing such requirements could be lacking the technical or job related understanding to write the ad. Which is understandable of course, as you can’t expect recruiters to be an expert in every field they hire candidates in. This is often the case when HR managers hand insufficient job requirements to recruiters, leaving them scrambling to make up what they think sounds good based on their very extremely limited knowledge.&lt;/p>
&lt;p>Hiring managers have gotten very picky, preferring to find the candidate with the required experience rather than investing in training. This is in turn decreasing employee loyalty as employees no longer feel valued and appreciated which of courses increases turnover in a company. This of course puts companies behind the competition as employees flee towards jobs offering better benefits.&lt;/p>
&lt;p>Many recruiters are being put under unnecessary pressure which encourages them to write impossible job requirements in the search for the super candidate which doesn’t exist. This becomes very evident in job requirements which at times sound like they merge two professions together.&lt;/p>
&lt;p>Poorly written job ads may also be contributing to these insane requirements. It is often the case that rather than specifically requesting X amount of years of experience in a specific tool or technology, recruiters may simply be looking for a candidate having that amount of experience in that role, while also having knowledge of those tools or technologies. The wording of a job ad can make a huge difference, so it is important to proof read each and every one.&lt;/p>
&lt;p>Many hiring managers also intentionally request impossible job requirements to build a case for H-1B allotments, which are very hard to currently obtain. This allows hiring managers to make the case that there is no national that can fill the requirements for the job, as this person does not exist, making it easier for them to obtain work visas and hire international candidates with much lower salary expectations.&lt;/p>
&lt;h1 id="are-we-doomed">Are we doomed?&lt;/h1>
&lt;p>No, not really. However, changing things won’t be an easy fix as this will require many people within a company to change their mindset, and we all know how much people hate being told that what they believe is wrong.&lt;/p>
&lt;p>Recruiters should take a look at the job requirements and determine whether employees currently filling the same role in the company could have met the job requirements if they joined today. If the answer is no, which is the case most of the time, expectations need to be somewhat managed.&lt;/p>
&lt;p>Recruiters can still present hiring managers with candidate resumes they think are the right fit for the job. After all, the recruiter is meant to be a consultant, a counselor, an operator, and a salesman. It is the recruiter’s job to manage the expectations of the hiring manager, gently of course, by empathising with the pressure and uncertainty the hiring manager would be facing.&lt;/p>
&lt;p>More recruiters should be comparing their salary expectations against those currently being offered by competitors in the industry. It is essential that recruiters start to push for their companies to start participating in salary surveys that provide robust and detailed compensation data for all industries and geographies. How else are hiring managers going to realise that they’re asking for top talent without offering enough pay?&lt;/p>
&lt;p>Transparency is key. Although it is the recruiter’s job to understand the market better than the hiring manager, the hiring manager is always the key decider in the recruitment process. Recruiters must be vocal and always keep the hiring managers in the loop, always using data to back their claims.&lt;/p>
&lt;p>Invest in your employees. Not all candidates start off with all the experience the job requires. Companies that invest in their employees are more likely to respond to changes in the market.&lt;/p>
&lt;blockquote>
&lt;p>“The only thing worse than training your employees and having them leave is not training them and having them stay.” — Henry Ford&lt;/p>
&lt;/blockquote>
&lt;h1 id="what-can-we-do-in-the-meantime">What can we do in the meantime?&lt;/h1>
&lt;p>Just apply regardless. I like to use the 70% rule when looking for a job. If you feel that you meet 70% of the job requirements listed in the ad, consider applying. Recruiters could be trying to cast as wide a net as possible, which can easily backfire as qualified candidates easily get confused leaving them questioning whether they’re right for the job.&lt;/p>
&lt;p>You could also track down people who work at the company to get a feel on whether all the requirements are really required. LinkedIn could be an easy way of doing this and could even help you score referral points.&lt;/p>
&lt;p>Tailor your resume for every job you apply for. One size does not fit all, and tailoring your resume for the job you’re applying for can easily increase your chances of scoring an interview.&lt;/p>
&lt;p>Get in touch with the recruiter if you don’t hear anything from them after a month or two. They could either still be looking for candidates and had to lower their expectations which means that you would be reminding the recruiter that you’re still interested in the job, or they could even give you a reason as to why they turned you down for the job helping you reevaluate your job hunting strategies.&lt;/p>
&lt;p>After all, experience is not the only important thing for a candidate to thrive in a company. Soft skills, interpersonal skills, and being a team player all play a huge role, let’s not forget that. So even if you only meet 70% of what’s listed in the ad, you could still stand a reasonable shot at getting an offer.&lt;/p>
&lt;blockquote>
&lt;p>“You miss 100% of the shots you don’t take” — Wayne Gretzky&lt;/p>
&lt;/blockquote></content></entry><entry><title type="html">About</title><link href="https://gabriel.gaucimaistre.com/about/"/><id>https://gabriel.gaucimaistre.com/about/</id><author><name>Gabriel Gauci Maistre</name></author><published>2018-01-01T23:00:00+00:00</published><updated>2018-01-01T23:00:00+00:00</updated><content type="html" xml:base="https://gabriel.gaucimaistre.com/about/">&lt;p>&lt;em>The views and opinions on this website are my opinions and do not reflect the views of my current or former employers.&lt;/em>&lt;/p>
&lt;h2 id="short-biography">Short biography&lt;/h2>
&lt;p>I am a data scientist who is extremely passionate about being able to derive insight from data. I currently work for a leading car rental company in the automotive industry based in Munich, but curious to all fields where data has a huge impact.&lt;/p>
&lt;p>I have learnt a lot on my journey thus far, topics which include machine learning, deep learning, recommender systems, parallel computing, and high performance computing.&lt;/p>
&lt;p>I am constantly setting myself out to learn new things, be it from online articles and discussions, to books and podcasts. Many people do not understand why I do so, especially at late hours of the day, but it is very simple to me as learning new things is something that I find great joy in doing.&lt;/p>
&lt;h2 id="open-source-projects">Open source projects&lt;/h2>
&lt;p>&lt;a href="https://github.com/gabegm/Predict-Market-Turnover">Predicting market turnover&lt;/a>&lt;/p>
&lt;p>I built a predictive model to forecast the turnover for the Bundesliga season 17/18 for a candidate test.&lt;/p>
&lt;p>&lt;a href="https://github.com/gabegm/Data-Driven-Automated-Algorithmic-Trading">Data Driven Automated Algorithmic Trading&lt;/a>&lt;/p>
&lt;p>For my thesis project, I built an automated algorithm to trade the markets.&lt;/p>
&lt;p>&lt;a href="https://github.com/gabegm/Headcount-Planning-Management-System">Headcount Planning Management System&lt;/a>&lt;/p>
&lt;p>Create effective hiring plans, identify gaps, and align with financial performance and business goals.&lt;/p>
&lt;p>&lt;a href="https://github.com/gabegm/ShoppingCart">Shopping Cart&lt;/a>&lt;/p>
&lt;p>An e-commerce website built on top of ASP.NET 4 MVC5.&lt;/p>
&lt;h2 id="long-form-biography">Long form biography&lt;/h2>
&lt;p>When I was about fourteen-years-old, I used to spend countless of hours playing a game called &amp;ldquo;Grand Theft Auto: San Andreas&amp;rdquo;. This game had a huge community behind it, many of who dedicated their free time to create modifications which either changed or extended the game beyond its capabilities. One day I found a modification which implemented online multiplayer features, something which the game lacked and was wished for greatly. I instantly got hooked, as from being used to playing on my own, I was suddenly opened up to thousands of people who I could play with, just thanks to a single modification. But this modification did a lot more than just provide online multiplayer features, it also allowed people to use a scripting language called Lua to change the behaviour of the game. Whether it was simply spawning something out of nowhere, or even created whole new game modes which people could play together, this scripting language could allow anyone to do it all. This amazed me of course, and got me interested in learning the language to be able to do all the cool things these other people were doing. A few months later, this online multiplayer modification went open source, an area completely new to me at the time and got me interested in open and collaborative programming. This is when I realised that programming was what I wanted to do out of college.&lt;/p>
&lt;p>In college, I discovered the field of data analytics. I liked the idea of being able to derive insight from raw data. I knew that I would want to get into this field but my bachelor degree did not have any units on data analytics. This is why I set it upon myself to learn and master the field on my own and base my thesis on it. I knew that if I were to do this, I would have something to show for my work. Having found the area of finance to also be interesting to me, I thought it would be a great idea to merge the two fields together. This is when I discovered the field of computational finance, a branch of applied computer science that deals with problems of practical interest in finance.&lt;/p>
&lt;p>While researching for a thesis topic, I found a particular field called algorithmic trading, a method of executing a large order using automated pre-programmed trading instructions accounting for variables such as time, price, and volume to send small slices of the order out to the market over time. Being challenged with the fact that I knew very little about finance, I set it upon myself to read heavily on many terms and techniques available to me online, including podcasts which I would listen to on my drive to college. This helped me greatly in getting up to speed on ideas which I could apply towards my thesis. I set out to learn python, which is a common high-level programming language used in this area, and built a fully automated strategy leveraging machine learning models to decide whether to trade certain US equities. Although this strategy did not beat the market having made a profit of over 100%, I was still pleased knowing that my first attempt could yield a profit at all, let one of 83%.&lt;/p>
&lt;p>After college, I joined a company as their first HR Data Analyst, a field that was completely new to me but a challenge I did not shy away from. Plagued with HR data not being unified across the departments in the HR group, I was tasked with handling not only the data from my department, but also that belonging to the whole group. I set up data entry standards, tidying checks, reporting, and visualisations, all of which fully automated. I was able to apply what I learnt for my thesis, and build tools which helped the business take back control of their data and understand it more through actionable insights. It was also my duty to handle the GDPR complaince for all HR departments across the group, a task which involved a lot of collaboration with various departments across various locations.&lt;/p>
&lt;h3 id="contact-me">Contact me&lt;/h3>
&lt;p>&lt;span class="e-mail" data-user="leirbag" data-website="moc.ertsiamicuag">&lt;/span>&lt;/p></content></entry></feed>